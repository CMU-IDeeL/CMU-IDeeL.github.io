projects =
[{
    title: "Video Super Resolution",
    team: "HowDareU",
    video: "https://youtu.be/wilLwo8ES5g",
    report: "https://drive.google.com/file/d/1oyjYKha0iqlRWRa61fffb1HHwPI4CJbp/view?usp=sharing",
    summary: "Most of us are young people born in the 1990s, a lot of classic anime like Slam Dunk, Bleach, Pokemon, Naruto, was the best memory in our childhood. But when we want to relive those animes, we find that the resolution is a little low comparing to those of today‚Äôs film. So, we try to use video super-resolution methods to rebuild those animes. Our project uses Your Name, which is directed by Makoto Shinkai as the data source. We build a Second-Order Attention Network to generate 4X size high-resolution frames from low-resolution ones and combine those high-resolution frames together into a video. The generated video is really amazing, each frame has clear lines, shapes. If you don‚Äôt look it closely, you may hardly distinguish it from the raw high-resolution video. We add 30 seconds cut in our presentation. We also build a super-resolution website: 54.193.204.208:3000/ for you to upload low-resolution anime images. Feel free to play it!",
    pic: "HowDareU.png",
},
{
    title: "Automatic Super Frame Rate",
    team: "Team 6, Again",
    video: "https://www.youtube.com/watch?v=sQfeFmtlY2E",
    report: "https://drive.google.com/file/d/17hPxhh3wcWxtkIa3ex4BeSA40oGhGgo_/view?usp=sharing",
    summary: "In this project, we will examine how various deep neural networks can be applied to the production of Anime videos. This distinctive video genre originated in Japan and now fuels a thriving industry with anime movies and TV series growing popular worldwide. However, these beautiful animations are still drawn manually with painstaking human effort frame by frame. The aim of the paper is to increase the frame rate of the Japanese animated movie \"Your Name\" or \"Kimi No Na Wa\".Produced by CoMix Wave Films. To do this, a plausible intermediate frame needs to be interpolated between two existing frames. This would greatly reduce the workload of the artist. We propose to use a Beta Variational Auto Encoder model (VAE) to identify scenes changes and an Adversarially Constrained Autoencoder Interpolation model (ACAI) to do interpolation. With a tool like this, an artist could draw only a few frames from a scene and still reach their desired framerate. In the future, we hope to combine the VAE and ACAI to intelligently detect and prevent interpolation between two different scenes.",
    pic: "Team6Again.png",
},
{
    title: "3D Object Inpainting Using GANs",
    team: "Shallow Learners",
    video: "https://youtu.be/2ur5vF4htEw",
    report: "https://drive.google.com/file/d/1nuZsYHVlByTbfjVEcLe92E74kzEVg3wR/view?usp=sharing",
    summary: "Inpainting is a process of restorative conservation where damaged, deteriorating, or missing parts of an artwork are reconstructed. In computer vision, while most of the traditional methods focus on 2D image inpainting, the fast development of 3D perception technologies has significantly increased demand for 3D inpainting. Several ongoing world-wide research domains, such as self-driving and 3D mapping, are suffering from uncompleted 3D shapes in their point cloud data. \nTo solve such problems, we proposed a novel 3D inpainting method utilizing GANs to complete missing parts of an object in its point cloud representation. During experiments on the ShapeNet dataset, our methods obtained comparable results to the state-of-the-art algorithms in the studied object categories. The generated points well resemble the ground truth in terms of contour and distribution.",
    pic: "ShallowLearners.png",
},
{
    title: "Cascading Convolutional Neural Networks",
    team: "Data eVaders",
    video: "https://www.youtube.com/watch?v=dBDZ6R4_IhE&feature=youtu.be",
    report: "https://drive.google.com/file/d/1RXi2jKCYb7BSLXklBFN6rPEjo81cdEW1/view?usp=sharing",
    summary: "Typically, most neural network architectures are manually designed, by optimizing various hyperparameters, to model the underlying information. Since all the hidden units in a network are trained simultaneously, such architectures suffer from the problem of moving targets which results in a sub optimal network topology. Cascading Correlation (or CASCOR) algorithm addresses this problem by automatically training and adding a single hidden unit progressively thereby creating an optimized network topology. \nWith the benefit of parameter sharing and shift-in-variance, Convolution Neural Nets have shown huge success in image and audio classification with less number of parameters compared to feed-forward nets. However, training and optimizing the CNN architecture takes time and experience. Similar to feed-forward networks, the Cascade correlation learning algorithm can be employed to optimize a convolutional architecture. So, our objective is to develop a novel Convolution version of Cascading Neural Net to capitalize on the benefits of Convolution architecture and Cascading Correlation and benchmark the developed algorithm by performing image classification tasks on MNIST and CIFAR-10 datasets. \nIn this project, we propose a novel method to implement convolutional CASCOR, where the training begins with a minimal feed-forward network with flattened input, and then a new hidden unit is added which is a combination of a kernel of fixed size and a perceptron. New hidden units are added one by one, creating a multi-layer structure, until the error is acceptably small. We have performed experiments with the proposed architecture and the results have been presented.",
    pic: "DataeVaders.png",
},
{
    title: "Multiple Choice Question Answering",
    team: "WuhanJiayou",
    video: "https://www.youtube.com/watch?v=NfbCeMsDOYE&feature=youtu.be",
    report: "https://drive.google.com/file/d/1U1I4Pgnc1KJOtprAL9c_l9Uc6tORroUT/view?usp=sharing",
    summary: "How to boil eggs? While being a ridiculously easy question for humans to answer, it is actually quite challenging for the models. To answer this question, the model need to know that you will need a container to put the eggs in, some water and a heat source to boil. In other words, it requires commonsense about the physical properties of various objects and correct action sequence to perform certain activities. In this project, we try develop models to solve challenging question like this one. We proposed to search for relevant information about the question from different textual resources and knowledge bases and augmented the strong baseline with these additional information. We also incorporated a mechanism in our model to better mimic how human answer multiple choice questions. Our results suggest that our model has achieved the state-of-the-art performance on the recently released PIQA dataset.",
    pic: "WuhanJiayou.png",
},
{
    title: "Accounting Fraud Detection with VAE-GAN",
    team: "Alphaholics",
    video: "https://youtu.be/mFxEvaG36rU",
    report: "https://drive.google.com/file/d/1TSwxivv41SxTISfGxZjqfIz2hRCelvc7/view?usp=sharing",
    summary: "The detection of fraud in accounting and transaction data is a crucial technique in asset management and investment decision making. Seeing that most modern detection procedures involve human analysts' hard work, we design and implement a new deep learning framework that can automatically detect the potential financial frauds given transaction-level data. We propose a new VAE-GAN based clustering algorithm with a classification methodology as improvements on the previously proposed adversarial autoencoder (AAE) framework, and our new VAE-GAN framework outperforms previous state-of-art approach in financial fraud detection.",
    pic: "Alphaholics.png",
},
{
    title: "SimBART: A New Approach to Text Summarization",
    team: "RandomForest",
    video: "https://www.youtube.com/watch?v=8X5toPmRnDs",
    report: "https://drive.google.com/file/d/1gUBjIrnarh3u_IidBnbX_lZEFnS0-XJx/view?usp=sharing",
    summary: "Our project focuses on text summarization, which refers to generating a short summary consisting of a few sentences that capture salient ideas of a text. The mostly used maximum-likelihood method which minimizes cross-entropy loss, may not be appropriate for summarization task since such a loss is too strict to account multiple valid answers. Therefore, we build our own SimBART model based on BART model using our own SimScore. The traditional similarity score cannot be used as a loss function because it is not a differentiable process. Our approach is to conduct a two-stage computation so that the score can differentiate with respect to the parameter in the summarization model. Our experimental results show that our SimBART which based on our SimScore can outperform other benchmark models such as BART and BiLSTM-PN-W2V in some popular evaluation measures, indicating that our SemScore is a good improvement of the traditional negative log-likelihood loss and can better update the parameters in the summarization model such as BART.",
    pic: "RandomForest.png",
},
{
    title: "STAPLE: Simultaneous Translation and Paraphrasing for Language Education",
    team: "LocalMinima",
    video: "https://youtu.be/yExMGOoaEII",
    report: "https://drive.google.com/file/d/14ivkFMYb85cZxPkNOaHUot-952s-iAAy/view?usp=sharing",
    summary: "We introduce a system built for the Duolingo Simultaneous Translation And Paraphrase for Language Education task (https://sharedtask.duolingo.com/). State of the art machine translation systems typically produce a single output, but in certain cases it is desirable to have many possible translations of a given input text. Accordingly, given an English sentence, our system can produce a high-coverage set of translations. \nWe split the problem into a translation and a paraphrasing task. For translation, we use a Transformer based architecture, fine-tuning a model pre-trained on a large English-Japanese corpus. To improve coverage over correct translations, we found it effective to encourage output diversity with 'Diverse Beam Search‚Äô. For paraphrasing, we use a seq2seq LSTM model trained with a discriminator and global loss and uses submodular optimization-based decoding. \nWe find that correct variations of translations are relatively homogenous. Imposing excess diversity by penalizing (Diverse Beam Search) or optimizing for it (Paraphrasing models) is detrimental. The best results are obtained by very modest use of Diverse Beam Search and, if scores are available for each translation, exposing the model to 'better' translations more often. Our best result of 27.17% weighted F1 score outperforms Duolingo‚Äôs baseline score of 4.31%.",
    pic: "LocalMinima.png",
},
{
    title: "Semi-supervised Monocular Relative Depth Estimation",
    team: "DeepEnsemble",
    video: "https://youtu.be/l7LzEW9_Rk0",
    report: "https://drive.google.com/file/d/1yFwpwnbgCMSYCWdmn3wbp_HCjOpzbmIG/view?usp=sharing",
    summary: "Recovering depth information from a single image has potential applications in 3D reconstruction, robotic perception, and understanding the spatial layout of a scene. However, designing a single model for depth perception germane to indoor and outdoor scenes, and making the model independent of camera attributes remains a challenging task to this day. In this work, we intend to focus on depth estimation from a single image of an unconstrained setting with relative depth labels and enhancing the depth predictions by making use of semantic segmentation. We employ an attention mechanism to help the model focus on a particular aspect of the image. We use semantic image segmentation as an auxiliary task to improve depth perception in the wild. The loss of this auxiliary network is combined with the original depth perception loss to improve generalization. Finally, we wish to reinforce depth perception in parallel with image segmentation to enhance the performance.",
    pic: "DeepEnsemble.png",
},
{
    title: "Ambiguity in Creative Practices",
    team: "AmbiguousAI",
    video: "https://youtu.be/EZcyM_5vexE",
    report: "https://drive.google.com/file/d/1EUkUDUhoEjsJqmSLd-f7oMerTrTTCBQa/view?usp=sharing",
    summary: "Language is ambiguous. Through drastically different expressions one can convey the same idea. For experts, language engenders ‚Äúencoding‚Äù to a higher-level of technicality; for non-experts, language means understanding, that is ‚Äúdecoding‚Äù, into more conventional terms. Throughout Fine Art and Architectural history, design-based communication has followed in an opposite direction‚Äî humans have learned how to use complex tools (ranging from analogue devices such as the ruler-compass to digital devices such as computational design systems) in order to map intellectual ideas to a more general public understanding. \nHowever, conversations and descriptions about art, design or architecture, are yet full of vague and ambiguous terms. As of the day of writing, it is impossible to design directly through natural language. \nHypothesis: immerse in the digital era and with more data than we need for first time in the history, the use of a data-driven framework through deep learning and natural language processing algorithms will provide the necessary state-of-the-art components that will democratize digital creativity, art and design. Software skills for creative fields won‚Äôt be as necessary as today. \nAs part of a bigger research, out of the scope of this course, we want to open a conversation about the future role of creative practitioners. If a tool is able to give the ability to design to unskilled people, the role of the architect, designer or artist might shift to a more curatorial paradigm.",
    pic: "AmbiguousAI.png",
},
{
    title: "End-to-End 2D to 3D Video Conversion",
    team: "FunnectionistML",
    video: "https://youtu.be/KuyQGW0r4RE",
    report: "https://drive.google.com/file/d/1rMTUX65g9kUlCrJDC4z4LHZnwCydrwYI/view?usp=sharing",
    summary: "Our project deals with the problem of automatic 2D to 3D video conversion in an end-to-end manner using deep learning. The value to solving this problem is immense. 2D to 3D video conversion will enable a movie shot in 2D to be viewed in 3D, thus eliminating the need for expensive 3D videography equipment. We leverage the power of deep learning to predict the right image of a stereo image pair given the left image. The model applies a set of probability maps to horizontally shifted versions of the left image to produce the right image. We further infuse depth and segmentation information in order to make our predictions better.",
    pic: "FunnectionistML.png",
},
{
    title: "Online COmment Toxicity Detection and Classification",
    team: "Shrinkage",
    video: "https://youtu.be/CtSbP7qL_Ps",
    report: "https://drive.google.com/file/d/1_rspNtAsnyojRdIdZ7xTxapndX0MJUGo/view?usp=sharing",
    summary: "Our project focused on the use of the BERT NLP model to identify and label toxic comments online. Discussing things you care about online can be difficult. Users may be scared to comment online in fear of being attacked and ridiculed. Therefore, it is important to identify and moderate these comments. However, due to the number of comments posted, doing it manually is difficult. Companies like google are already trying to solve this issue with AI and have had good results, but even they have issues making good classification. For example, if group identities that are often attacked such as ‚Äúblack‚Äù or ‚Äúgay‚Äù appear in harmless comments like ‚ÄúI am a black man,‚Äù they will be flagged as toxic. Our model was able to give a comment an overall toxicity score as well as classify it into subtypes such as ‚Äúinsult‚Äù or ‚Äúsexually explicit.‚Äù Even better, our model was able to identify and classify toxic comments while performing better than Perspective Al since ours is less likely to label harmless comments mentioning identities as toxic.",
    pic: "Shrinkage.png",
},
{
    title: "Lottery Ticket Hypothesis",
    team: "Dropouts",
    video: "https://youtu.be/AbuAyJ5T2us",
    report: "https://drive.google.com/file/d/12X3mCAsQTnyh0p8kzb3Joi5v6pCw0nNq/view?usp=sharing",
    summary: "As useful and efficient the deep learning networks are, ever since their boom, there has been a significant increase in the complexity and size of the deep learning models. To make deep learning more accessible in computationally restricted scenarios like autonomous driving, etc scientists have been using pruning to get similar accuracy but with a much smaller model. \nIn our project, we analyze the Lottery Ticket Hypothesis on a rather complex network - Fastdepth and see its impacts with different initialization, learning rate warm-up, etc. \nLottery Ticket Hypothesis states \"A randomly-initialized, dense neural network contains a sub-network that is initialized such that, when trained in isolation, it can match the test accuracy of the original network after training for at most the same number of iterations\". \nWe also compare Lottery ticket hypothesis with other pruning methods available like Net Adapt. Such pruned network will allow such models to be used on smaller machine and is vital in bringing complex deep learning network to reality.",
    pic: "Dropouts.png",
},
{
    title: "RL Agents for Optimal Sequential Decision-Making of Cyber Threat Mitigation",
    team: "Cybersecurity",
    video: "https://www.youtube.com/watch?v=ZYmzeH5nFKU",
    report: "https://drive.google.com/file/d/17Tw3TdJ8a0R1MeU97vDJwY7hnLa6ETca/view?usp=sharing",
    summary: "In 2017, 68% of cyber attacks went undiscovered and 77% of these attacks took hours to days to get mitigated. The lack of timely response is mainly caused by the insufficient support of attack mitigation policies that allow for dynamic and automated intrusion response. The automation of threat mitigation is very important to make real-time decisions against cyber attacks. In this project we develop an automated decision-making agent using Reinforcement Learning to observe and respond to cyber intrusions accurately and timely. We used the MITRE ATT&CK Framework, an actionable repository of adversarial tactics, techniques, and mitigations to develop a Deep Q-Learning network to find the optimal mitigation policy for any cyber attack. Our developed Deep Q-learning network used Experience Replay to consider the full history of the attack chain, then predicted and mitigated the next technique the adversary will perform in the sequence. Our Reinforcement Learning defense agent successfully created better optimal mitigation policies than the ordinary Q-learning algorithm and baseline policies.",
    pic: "Cybersecurity.png",
},
{
    title: "Depth Mask R-CNN",
    team: "Incognito",
    video: "https://www.youtube.com/watch?v=PHWmLmspA4A",
    report: "https://drive.google.com/file/d/1s_YSG5Ii5fUw3Mk_B4c6l4zrg8_Mqjs3/view?usp=sharing",
    summary: "Monocular depth estimation is the problem of estimating depth from a single RGB image, which is a very challenging task. Monocular depth estimation can be used in Augmented Reality (AR). With the popularity of online shopping, it is possible to bring dresses and other commodities to the people by AR. Also, sports can be rendered on the living room table top and watched live. To build these AR rendering, we need to estimate depth of the images/video frames. \nThere is a current trend in Computer Vision, to move towards joint learning. For e.g., Object Detection and Segmentation are performed jointly so that each task learns from the other. As a result both tasks improve. Inspired by the above two trends, in this project we plan to estimate the depth of a monocular image using a deep neural network. We aim to formulate this depth estimation task as part of a multi-task framework. Specifically, we plan to solve three tasks in parallel, namely, object detection, semantic segmentation, and depth estimation. \nIn conclusion, our model completes three tasks in real-time where each task improves the other.",
    pic: "Incognito.png",
},
{
    title: "Explorations into Adversarial Robustness",
    team: "Robustify",
    video: "https://youtu.be/21ii2dS1cVQ",
    report: "https://drive.google.com/file/d/1hf6bWB2eV27zInTSmQQBlENGTM5aZ6iY/view?usp=sharing",
    summary: "Artificial Intelligence (AI) is in the process of revolutionizing the way that we live. Already we can ask our phones questions, recognize faces quickly and autonomously, and are on our way to fully robotic surgeries. Much of these advances have been powered by neural networks. As we put more collective trust in these networks, bad actors are increasingly developing new ways to fool neural nets in ways imperceptible to humans. Consider the example of an image of a person that the network thinks is a green light and the implications. \nRobustify is trying to help solve this problem by designing training mechanisms for neural networks that make them robust to such adversarial examples. This may also provide interesting insight into how machines learn, which is currently opaque.",
    pic: "Robustify.png",
},
{
    title: "Spatio-Temporal Action Recognition",
    team: "Munich",
    video: "https://youtu.be/OXI8Xsj0qXw",
    report: "https://drive.google.com/file/d/1Ji-pQjFatHkkOEXT_MhCtN1X4F0c2-XE/view?usp=sharing",
    summary: "Spatiotemporal action recognition is the task of locating and classifying actions in videos. Our project applies this task to analyzing video footage of restaurant workers preparing food, for which potential applications include automated checkout and inventory management. Such videos are quite different from the standardized datasets that researchers are used to, as they involve small objects, rapid actions, and notoriously unbalanced data classes. We explore two approaches ‚Äì one involving the familiar object detector ‚ÄúYou Only Look Once‚Äù (YOLO), and another applying a recently proposed analogue for action recognition, ‚ÄúYou Only Watch Once‚Äù (YOWO). In the first, we design and implement a novel, recurrent modification of YOLO using convolutional LSTMs and explore the various subtleties in the training of such a network. In the second, we study the ability of YOWO‚Äôs three-dimensional convolutions to capture the spatiotemporal features of our unique dataset, which was generously lent by CMU-based startup Agot.",
    pic: "Munich.png",
},
{
   title: "Generative Deep Learning for High Rise Building Design",
   team: "Archi-Learning",
    video: "https://vimeo.com/409341878",
    report: "https://drive.google.com/file/d/14GXSWvLcVgoFOVVEVCa7QssWkjQ5iSI0/view?usp=sharing",
    summary: "This research proposes a new way to model and design a high-rise building form by learning the morphological features that correspond to its own architectural design principles through deep neural networks. By collecting and training diverse forms of high-rise building data from major cities in the world, we train a deep generative model and prototype a system for generating a high-rise building form in three different ways: exploration, synthesis, and interpolation. This research demonstrates that a deep generative model can grasp highly complex principles of architectural form and that design decisions for new forms can be processed based on deep learning of data. By further developing a generated form into schematic design, the system provides an example of a high-rise building design with a style distinctive to the deep generative model.",
   pic: "Archi-Learning.png",
},
{
    title: "Spatio-Temporal Video Question Answering",
    team: "HouseNNet",
    video: "https://youtu.be/fwdH6UdnKF4",
    report: "https://drive.google.com/file/d/18qpVyllw4zJ3lM7eMLfxk2ckiNOFmrkS/view?usp=sharing",
    summary: "How often have you found yourself wishing you could query a video? What did the professor say about the breadth of a neural network? What instrument did the surgeon use to remove the tumor? Who‚Äôs sitting at Sheldon‚Äôs spot? The use cases are endless, and with an increasing amount of content being pushed onto the internet we need to able to query and interact with video content better. TVQA+ presents a novel approach to perform Spatio-Temporal Video Question Answering (STAGE) allowing to identify the relevant timespan in a video, objects that inform and select the most suitable answer for the question being posed by the user. The STAGE model presents a Deep Neural Network that leverages contextual embeddings, convolutional encoders, and self-attention across multiple modalities to tackle this problem. STAGE is currently the State-Of-The-Art solution for video question answering on the TVQA+ dataset with a Question Answering accuracy of 72%.",
    pic: "HouseNNet.png",
},
{
    title: "Scene Detection in 2D Animation using &beta;-VAE",
    team: "Ph‚Äôam",
    video: "https://youtu.be/aFOiCHoo4bA",
    report: "https://drive.google.com/file/d/1WiR71c4m-dF31N6Ctglsy-7TgMn7opke/view?usp=sharing",
    summary: "A 2D animation is handcrafted graphics that give a perception of movement in space. Popular Japanese anime like Naruto is a good example. Scene detection is identifying the time instance when there is a change in the visual in the anime. This remains a crucial problem for any task that involves anime for example: interpolating anime frames. Till date, this has been majorly done by investing hours of manual work. We propose an AI based approach which can automate and fasten this painful process. Intuitively, this would mean we will have to learn the image distribution and somehow classify images as points of scene detection or not. \nVariational Autoencoders are very popular when it comes to learning data distribution. Hence, it was an obvious choice given they are easier to train as well. We trained a ùú∑-VAE to learn the data distribution and used the notion: a high KL Divergence between image pairs identifies a scene change.",
    pic: "Ph'am.png",
},
{
    title: "DeepFake Video Detection",
    team: "Overtrained",
    video: "https://www.youtube.com/watch?v=DZkNki3vSOc",
    report: "https://drive.google.com/file/d/1_QMkzpCu8DotA5shXSsq91b4BljWcOza/view?usp=sharing",
    summary: "Deep learning advances have been applied to solve many real-world problems that are close to people‚Äôs daily life. Recently, DeepFake videos, generated by deep learning models usually using generative adversarial networks and auto-encoders, have caused privacy concerns in society. Our project addressed these problems by experimenting with data augmentation methods and a multimodal model that distinguishes fake videos from the original ones. Our baseline architecture includes a face detector, an embedding extractor and a deep learning classification model. We improved the baseline by experimenting on data augmentation methods in order to enhance the frame-level features. The result showed improvements compared to the baseline. Furthermore, we introduced audio-level features to the model. Previous works related to DeepFake video detection normally focused on the frame-level features extracted from video contents. Therefore, we proposed a multimodal network that used early fusion to combine the frame and audio features. The result showed an improvement in classification performance and a smoother training process.",
    pic: "Overtrained.png",
},
{
    title: "Super Frame-Rate on Anime",
    team: "Pepe",
    video: "https://youtu.be/R_8TShW7EZI",
    report: "https://drive.google.com/file/d/1WFuhveq6MXr0lY5Y5cbUG-cH4sveg8-j/view?usp=sharing",
    summary: "In a consecutive set of frames such as a video, it would be a huge problem if some of the frames are missing. The video without several images in between existing images will disturb people from enjoying the content because it will often give black-outs. If the video doesn't have image data to render, then it is natural to produce black screen. To solve this serious problem, our team applied a deep learning model to existing frames and seamlessly create the missing frames. We used a popular Japanese anime in our work, Your Name(Kimi no Na wa), take image frames from this animation and pushed to our model to see whether the model infers missing parts correctly with existing images. Our team compared the original anime to the newly created anime from our model, and were impressed by the result that the deep learning model well-produced the missing frames by utilizing other existing frames. We believe our model can resolve this common disturbing problem, making many anime viewers enjoy without being annoyed of black-out problems.",
    pic: "Pepe.png",
},
{
    title: "Anime Scene Identification and Super-Resolution",
    team: "SafeAndSound",
    video: "https://youtu.be/_904Q30I7BE",
    report: "https://drive.google.com/file/d/1Hxcy2hwu_c_-HBhppJnrpJzNp23EPm86/view?usp=sharing",
    summary: "Some of us may view 2D animation in our free time. Each frame of the 2D animation is drawn by artists and they are limited by the resolution of their saved product. While watching the animations, the audience still wants to see the beauty in the details of every frame. Here come the problems of whether we could understand the information in the 2D animation and use the information to find scene changes. Are we able to use the information to provide anime with higher quality, for instance, to increase the resolution? To answer the first question, we proposed a beta VAE model to see the distribution of an image and then use the information to detect the scene change. After solving this problem, we enabled video super-resolution of 2D animated as much as 4 times the original resolution through the deep and shallow convolutional network for a 2D animation, ‚ÄúYour Name‚Äù.",
    pic: "SafeAndSound.png",
},
{
    title: "Hidden Fashion",
    team: "ShallowLearning",
    video: "https://youtu.be/CezK-SCXwsg",
    report: "https://drive.google.com/file/d/1F_XSHVJ1v62Sf3prWMQqLV_flQpaDDy2/view?usp=sharing",
    summary: "Fashion image retrieval between customers and online shopping stores has various applications for E-commerce. Deepfashion2 contains 491K images of 13 popular categories and 873K Commercial-Consumer clothes pairs. Its rich annotations support multiple tasks of fashion understanding such as clothes detection and classification cross-domain instance-level clothes retrieval. Our tasks include three tasks: landmark estimation, category classification and clothes retrieval. To realize this, we re-implemented the Mask-R-CNN model as the baseline, and designed multiple models including model based on VGG-16, Mobilenet V2.Finally, we proposed our model Mag-R-CNN based on Mask-R-CNN. Our main contributions are (1) Enhance the ability of feature extraction by designing Resnet101-FPN network (2) Resize and magnify key points to improve detection ability. (3) Establish a jointly learned training framework. Top k(k=1,10,20) accuracy are conducted as evaluation. Results show that our model improved the baseline by 3.66% for retrieval accuracy. We hope our model could be applied in differentiating the genuine knockoffs or change the clothes virtually.",
    pic: "ShallowLearning.png",
},
{
    title: "Domino - Causal Extraction System",
    team: "IDLI",
    video: "https://youtu.be/OgfpWw3aB18",
    report: "https://drive.google.com/file/d/1zmUGxvBFi0Xp1WK8JrOYHPutCVYdhkve/view?usp=sharing",
    summary: "Automatic identification of cause-effect relationships from data is a challenging but important problem in artificial intelligence. Identifying semantic relationships has become increasingly important for multiple downstream applications like Question Answering, Information Retrieval, Event Prediction. In this work, we tackle the problem of causal relationship extraction from financial news. Given the exponential growth in the number of articles produced everyday, it is imperative to be able to automatically detect important events from news to aid efficient ingestion of financial data for downstream applications such as stock trading and narrative summary generation. We propose two tasks - Identifying if a causal relationship is contained in a piece of text (A) and identifying segments which correspond to the cause and those that correspond to the effect (B). We employ a CNN and FinBERT based model for Task A. For task B we present two models - QA Based and Sequence Labeling based BERT model. On both these tasks we are able to surpass the current State of the Art. We also provide a detailed error analysis and briefly discuss future improvements.",
    pic: "IDLI.png",
}]
