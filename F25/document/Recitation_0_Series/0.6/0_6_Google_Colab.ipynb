{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyWWQGxVb9sF"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
        "https://colab.research.google.com/github/CMU-IDeeL/CMU-IDeeL.github.io/blob/master/F25/document/Recitation_0_Series/0.6/0_6_Google_Colab.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJFOWKUFYoYk"
      },
      "source": [
        "# Recitation 0: Introduction to Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6ASwJPOmB2_"
      },
      "source": [
        "# What's in this video?\n",
        "\n",
        "- Basics of Google Colab and it's Overview\n",
        "- Bash and Magic Commands\n",
        "- Session and Runtime\n",
        "- Managing your files using Google Drive\n",
        "- Saving and Loading Model Checkpoints\n",
        "- Managing Dataset\n",
        "- Colab Pro or Colab Pro+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x-Ov_cqYtZh"
      },
      "source": [
        "# Basics\n",
        "\n",
        "#### Google Colab\n",
        "\n",
        "- Colab is developed by Google Research and provides a Jupyter Notebook-style Python execution environment accessible directly through a web browser.\n",
        "- Main benefit is its computing resources.\n",
        "- For free, you get access to CPU and Tesla T4 GPU.\n",
        "- To access more powerful GPUs like L4 and A100, you can choose to pay for Google Colab Pro or Pro+ (https://colab.research.google.com/signup)\n",
        "\n",
        "#### Accessing Colab\n",
        "- Go to https://colab.research.google.com/ to create and access your notebooks\n",
        "- Directly from Google Drive\n",
        "- From your GitHub repository\n",
        "- Upload from local system\n",
        "\n",
        "This recitation assumes basic knowledge of using Jupyter Notebooks, so please familiarize yourself with it if you haven't already.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVcR53o6Yw16"
      },
      "source": [
        "# Bash and Magic Commands\n",
        "\n",
        "Colab runs in a linux environemnt and you can access the terminal with `!`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmIYabvTZg8O"
      },
      "source": [
        "#### Bash Commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1UlAjN-Dhzh"
      },
      "source": [
        "The !nvidia-smi command displays real-time GPU information, including\n",
        "- the GPU model (like Tesla T4 or A100),\n",
        "- memory usage (used vs available),\n",
        "- GPU utilization to show whether your code is actively using the GPU.\n",
        "- temperature\n",
        "- driver and CUDA version, which is helpful in ensuring compatibility with PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nau1JGAEYZk3",
        "outputId": "3c10324a-ff72-487b-bac4-bcb4217d500a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jul 14 05:04:36 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             46W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByRjEG-CY31w",
        "outputId": "f64129a3-c42e-4ab4-d80c-888b0568bb45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m221.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "import gc\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UmUuQlsZXlw",
        "outputId": "2e086dd0-f11b-4b10-e3d9-c621b7f1fcac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls\n",
        "# !cd ..\n",
        "# !mkdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmIqsLadZbvo"
      },
      "source": [
        "#### Magic commands\n",
        "- %time: only works on CPU commands.\n",
        "- For GPU, timing elapsed for operation is harder to measure, putting manual lines like\n",
        "- start = time.time()\n",
        "- ...(Your code)\n",
        "- end = time.time()\n",
        "- elapsed = end - start would generally be a rule of thumb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38cJEbCqDK1u"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfRtKSPCZeO-",
        "outputId": "3f8f09b3-9967-4397-964c-440de97b9a29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 7.31 ms, sys: 61 µs, total: 7.38 ms\n",
            "Wall time: 7.33 ms\n",
            "CPU times: user 9.79 ms, sys: 1.95 ms, total: 11.7 ms\n",
            "Wall time: 11.7 ms\n"
          ]
        }
      ],
      "source": [
        "%time result = [x**2 for x in range(100000)]\n",
        "%time result = list(map(lambda x: x**2, range(100000)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oqX33rSZkwM"
      },
      "source": [
        "# Runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z-2NXx1ZrH_",
        "outputId": "50b5056a-8fac-4c74-b9ad-d7a3bcf09d57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jul 14 05:05:02 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             46W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "import torch\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOFlPFPmZqYT"
      },
      "source": [
        "### Utilizing Free GPU/TPU Resources\n",
        "\n",
        "#### Changing Runtime\n",
        "- Runtime > Change runtime type\n",
        "- Select GPU/TPU and High-RAM option\n",
        "\n",
        "\n",
        "\n",
        "#### GPUs: Training Time of ResNet50\n",
        "- T4: 1x Speedup (Baseline)\n",
        "- V100: 3.6x Speedup (Comparing to T4)\n",
        "- A100: 10x Speedup (Comparing to T4)\n",
        "- TPU: TPU is a completely different architecture and require many training constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxyqXtzqmn8x"
      },
      "source": [
        "### Restart Session vs Restart Runtime\n",
        "\n",
        "\n",
        "Restart session\n",
        "- Close your browser session, with Colab Backend. Similar with closing a Jupyter Notebook tab.\n",
        "- Runtime > Restart session\n",
        "- Clears all session variables\n",
        "\n",
        "Restart runtime - disconnects cloud-based VMs in backend\n",
        "- It frees up resources and terminates all variables, files, and memory.\n",
        "- Runtime > Disconnect and delete runtime\n",
        "- Deletes session\n",
        "- Lose files in content folder\n",
        "- Switching GPUs will also delete current runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSyEXKxDmrEB",
        "outputId": "66bbcc31-77b3-4a90-9a0d-3e2fbdc8d623"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()  # Clear unused GPU memory cached by PyTorch to free up space.\n",
        "gc.collect()  # Call the Python's garbage collector to release unused memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efc0e4GeZvoQ"
      },
      "source": [
        "# Sample Helpful Code Snippets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_yKKFNPZzeW"
      },
      "source": [
        "### Mounting to Google Drive\n",
        "\n",
        "Very useful as you lose all files after the runtime ends, Because Colab’s local runtime is temporary — when the session disconnects or the VM resets, all files in /content are lost. Mounting command will give Colab notebook **access** to your Google Drive.\n",
        "After mounting, you can read/write files to paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2rPwLUyZx8r",
        "outputId": "d22fe316-16e0-4060-c001-ca36e011ffd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBjDQlUeZ1Hj"
      },
      "source": [
        "### Saving/Loading files - Model checkpoints\n",
        "\n",
        "A checkpoint is a saved snapshot of your model’s **state** at a particular point in training.\n",
        "It lets you resume training later or reload the model for inference — even if the Colab session crashes or disconnects.\n",
        "\n",
        "Checkpoints typically save\n",
        "- Model parameters (weights)\n",
        "- Optimizer, scheduler (name, learning rate)\n",
        "- Loss (metric that model aims to minimize, which calculates how wrong a model's prediction is compared to the true value)\n",
        "- Epoch/step number (how far along we are)\n",
        "\n",
        "In the next section, we are saving the model weights, optimizer state, scheduler state, training epoch, and metrics into a checkpoint file on Google Drive.\n",
        "\n",
        "The model weights capture what the model has learned so far, while the optimizer and scheduler states ensure that learning can resume with the exact same configuration. The epoch tells us how far along training was, and the metrics (like accuracy) help track performance at that point.\n",
        "\n",
        "All of this is bundled into a `.pt` file so that we can reload it later and pick up exactly where we left off.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqVTDvWxZ2Yo"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = []\n",
        "        for in_dim, out_dim in zip(size[:-2], size[1:-1]):\n",
        "          self.layers.extend([\n",
        "              nn.Linear(in_dim, out_dim),\n",
        "              nn.ReLU(),\n",
        "              nn.BatchNorm1d(out_dim),\n",
        "              nn.Dropout(0.5),\n",
        "        ])\n",
        "        self.layers.append(nn.Linear(size[-2], size[-1]))\n",
        "        self.model = nn.Sequential(*self.layers)\n",
        "        self.model.apply(self.init_param)\n",
        "\n",
        "    def init_param(self, param):\n",
        "      if type(param) == nn.Linear:\n",
        "        nn.init.xavier_uniform_(param.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.model(x)\n",
        "\n",
        "# Define your model\n",
        "model = MLP([40, 2048, 512, 256, 71])\n",
        "\n",
        "# Define optimizer, scheduler, loss function, epochs, and other metrics\n",
        "epoch = 5\n",
        "metrics = {'accuracy': 0.85}\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6nh1LaocNbS"
      },
      "source": [
        "#### Checkpoint Saving and Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X5vhmWocLmX"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metrics, epoch, path):\n",
        "    \"\"\"\n",
        "    Saves the model and other related states to a checkpoint file.\n",
        "\n",
        "    Functionality:\n",
        "    - Saves the model's state dictionary, optimizer state, scheduler state,\n",
        "      metrics, and epoch to the specified file checkpoint path.\n",
        "    \"\"\"\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         'metric'                   : metrics,\n",
        "         'epoch'                    : epoch},\n",
        "         path)\n",
        "\n",
        "def load_model(model, optimizer=None, scheduler=None, path='./checkpoint.pth'):\n",
        "    \"\"\"\n",
        "    Loads the model and other related states from a checkpoint file.\n",
        "\n",
        "    Functionality:\n",
        "    - Loads the checkpoint from the specified file path using `torch.load`.\n",
        "    - Restores the model's state dictionary from the checkpoint.\n",
        "    - Optionally restores the optimizer and scheduler states if they are provided.\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(path, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    else:\n",
        "        optimizer = None\n",
        "    if scheduler is not None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    else:\n",
        "        scheduler = None\n",
        "    epoch = checkpoint['epoch']\n",
        "    metrics = checkpoint['metric']\n",
        "    return model, optimizer, scheduler, epoch, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDegbEHWgVj7"
      },
      "source": [
        "#### Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-thnl4qAchh_",
        "outputId": "49a662bf-abfd-45d4-d56f-adaac167d750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to /content/drive/MyDrive/Checkpoints/11785_f25_rec0_google_colab_checkpoint.pt\n"
          ]
        }
      ],
      "source": [
        "# Define the directory and checkpoint's file path.\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/Checkpoints'\n",
        "MODEL_SAVE_PATH = os.path.join(CHECKPOINT_DIR, '11785_f25_rec0_google_colab_checkpoint.pt')\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Save the model.\n",
        "save_model(model, optimizer, scheduler, metrics, epoch, MODEL_SAVE_PATH)\n",
        "print(f\"Model saved to {MODEL_SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB5DzvIBgav3"
      },
      "source": [
        "#### Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDhYkd7LceqZ",
        "outputId": "f57d6ace-7afc-46dc-cf46-6d2593759477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded. Resumed at epoch 5 with metrics: {'accuracy': 0.85}\n"
          ]
        }
      ],
      "source": [
        "# Create a new instance of the same model architecture.\n",
        "loaded_model = MLP([40, 2048, 512, 256, 71])\n",
        "\n",
        "# Load the model, optimizer, and other saved states.\n",
        "loaded_model, loaded_optimizer, loaded_scheduler, loaded_epoch, loaded_metrics = load_model(\n",
        "    loaded_model, optimizer, scheduler, MODEL_SAVE_PATH\n",
        ")\n",
        "\n",
        "# Verify the loaded states.\n",
        "print(f\"Model loaded. Resumed at epoch {loaded_epoch} with metrics: {loaded_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWjh3DouZ-58"
      },
      "source": [
        "### Managing dataset\n",
        "\n",
        "Obtaining dataset\n",
        "- Kaggle Command\n",
        "- Manually uploading\n",
        "- Download/uploading dataset every time\n",
        "- Move dataset from Google Drive into content folder\n",
        "- Connect to GCP or AWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glGuFriJZ_4n",
        "outputId": "319a1237-a6d3-4999-dc92-8c25c709ba83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaggle==1.5.8\n",
            "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73249 sha256=5831f813bc7a66e9c65b3df67bc4ac15f1b5295fa4af87fe32c9a5d40d3bbe55\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/23/bd/d33cbf399584fa44fa049711892d333954a50ed4b86948109e\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.7.4.5\n",
            "    Uninstalling kaggle-1.7.4.5:\n",
            "      Successfully uninstalled kaggle-1.7.4.5\n",
            "Successfully installed kaggle-1.5.8\n",
            "401 - Unauthorized\n",
            "unzip:  cannot find or open /content/11785-spring-25-hw-1-p-2.zip, /content/11785-spring-25-hw-1-p-2.zip.zip or /content/11785-spring-25-hw-1-p-2.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "# Downloads dataset from kaggle\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "# Retrieve the Kaggle Username and API key\n",
        "from google.colab import userdata\n",
        "kaggle_username = userdata.get('USER_NAME')\n",
        "kaggle_api_key = userdata.get('KAGGLE_API_KEY')\n",
        "\n",
        "# Creates Kaggle config file with retrieved username and API key\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write(f'{{\"username\":\"{kaggle_username}\",\"key\":\"{kaggle_api_key}\"}}')\n",
        "\n",
        "# Sets appropriate permissions for Kaggle config file\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# Make sure to join the competition on Kaggle before running this command!!\n",
        "# Downloads dataset of the competition using Kaggle API.\n",
        "!kaggle competitions download -c 11785-spring-25-hw-1-p-2\n",
        "\n",
        "# Unzips downloaded dataset into given directory folder.\n",
        "!unzip -qo /content/11785-spring-25-hw-1-p-2.zip -d '/content/11785-spring-25-hw-1-p-2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVTCHiaZofKD"
      },
      "source": [
        "# Important Considerations for Students\n",
        "\n",
        "- Session Timeout: Google Colab sessions may time out after a certain period of inactivity. To prevent this, remember to save your work frequently and consider using Colab Pro to extend session runtimes.\n",
        "\n",
        "- Limited Persistent Storage: While Google Colab saves your notebooks on Google Drive, the storage space is limited. Make sure to clean up unnecessary files or download your work to your local machine to free up space.\n",
        "\n",
        "- Resource Limits: Free Google Colab accounts have some resource limitations, such as GPU availability and maximum session runtimes. For resource-intensive projects, consider upgrading to Colab Pro for improved performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p59ke10aB0z"
      },
      "source": [
        "# Colab Pro\n",
        "\n",
        "- Longer session runtime, reducing risk of timeout\n",
        "- Priority access to GPU\n",
        "- Increased storage\n",
        "- Background Execution (Google Colab Pro+)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "k_yKKFNPZzeW",
        "gBjDQlUeZ1Hj"
      ],
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
