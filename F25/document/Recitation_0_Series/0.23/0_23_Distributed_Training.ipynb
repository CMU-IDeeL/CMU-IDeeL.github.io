{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
        "https://colab.research.google.com/github/CMU-IDeeL/CMU-IDeeL.github.io/blob/master/F25/document/Recitation_0_Series/0.23/0_23_Distributed_Training.ipynb)"
      ],
      "metadata": {
        "id": "xp9JmCISJF_j"
      },
      "id": "xp9JmCISJF_j"
    },
    {
      "id": "dd9b8ef0-5d69-44b7-8da9-10ec7306dd4d",
      "cell_type": "markdown",
      "source": [
        "# DataParallel Notebook\n",
        "-------------------------\n",
        "Inspired by: [Pytorch Data Parallelism Tutorial](https://docs.pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)"
      ],
      "metadata": {
        "id": "dd9b8ef0-5d69-44b7-8da9-10ec7306dd4d"
      }
    },
    {
      "id": "34c98ebd",
      "cell_type": "markdown",
      "source": [
        "# Imports and Initial Setup\n",
        "------------------------------------\n",
        "This cell imports the necessary PyTorch libraries."
      ],
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "34c98ebd"
      }
    },
    {
      "id": "6db1e4d5",
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T14:30:01.356521Z",
          "iopub.execute_input": "2025-06-18T14:30:01.357274Z",
          "iopub.status.idle": "2025-06-18T14:30:01.360775Z",
          "shell.execute_reply.started": "2025-06-18T14:30:01.357249Z",
          "shell.execute_reply": "2025-06-18T14:30:01.359986Z"
        },
        "id": "6db1e4d5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d6af9aca",
      "cell_type": "code",
      "source": [
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Check for available GPUs\n",
        "if torch.cuda.is_available():\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    print(f\"Found {num_gpus} GPUs.\")\n",
        "    # Set the primary device\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    num_gpus = 0\n",
        "    print(\"No GPUs found. Running on CPU.\")\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "lines_to_next_cell": 1,
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T14:30:21.609490Z",
          "iopub.execute_input": "2025-06-18T14:30:21.610011Z",
          "iopub.status.idle": "2025-06-18T14:30:21.615075Z",
          "shell.execute_reply.started": "2025-06-18T14:30:21.609987Z",
          "shell.execute_reply": "2025-06-18T14:30:21.614246Z"
        },
        "id": "d6af9aca",
        "outputId": "e0ebb557-e0c2-461c-fc16-169c04cb816d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "PyTorch Version: 2.6.0+cu124\n------------------------------\nFound 2 GPUs.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "0c00bc52-470a-4526-826e-2216ff467778",
      "cell_type": "markdown",
      "source": [
        "# Data Parallel\n",
        "-----------------------------\n",
        "Source: [DataParallel vs. DistributedDataParallel in PyTorch: Whatâ€™s the Difference?](https://medium.com/@mlshark/dataparallel-vs-distributeddataparallel-in-pytorch-whats-the-difference-0af10bb43bc7)"
      ],
      "metadata": {
        "id": "0c00bc52-470a-4526-826e-2216ff467778"
      }
    },
    {
      "id": "b7715eb1",
      "cell_type": "markdown",
      "source": [
        "# Define a Simple Model\n",
        "-----------------------------\n",
        "We'll create a basic neural network for this demonstration.\n",
        "DataParallel will replicate this model on each available GPU."
      ],
      "metadata": {
        "id": "b7715eb1"
      }
    },
    {
      "id": "137aa2ce",
      "cell_type": "code",
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, output_size)\n",
        "\n",
        "    def forward(self, x, debug=False):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        if debug:\n",
        "            print(\"\\tInside the Model: input size\", x.size(), \"output size\", out.size())\n",
        "        return out"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T14:33:51.486118Z",
          "iopub.execute_input": "2025-06-18T14:33:51.487053Z",
          "iopub.status.idle": "2025-06-18T14:33:51.491916Z",
          "shell.execute_reply.started": "2025-06-18T14:33:51.487021Z",
          "shell.execute_reply": "2025-06-18T14:33:51.491377Z"
        },
        "id": "137aa2ce"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "66d4b247",
      "cell_type": "markdown",
      "source": [
        "# Data Preparation and Training Loop\n",
        "------------------------------------------\n",
        "This is the main part where we wrap our model with DataParallel\n",
        "and run the training process."
      ],
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "66d4b247"
      }
    },
    {
      "id": "84a4dd7e",
      "cell_type": "markdown",
      "source": [
        "## 1. Hyperparameters and Data"
      ],
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "84a4dd7e"
      }
    },
    {
      "id": "113e6748",
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "output_size = 10\n",
        "batch_size = 256  # A larger batch size helps utilize multiple GPUs\n",
        "learning_rate = 0.01\n",
        "num_epochs = 20\n",
        "\n",
        "# Create dummy data\n",
        "# We create a dataset of 10000 samples\n",
        "inputs = torch.randn(10000, input_size)\n",
        "targets = torch.randint(0, output_size, (10000,))\n",
        "\n",
        "# Use DataLoader for batching\n",
        "dataset = TensorDataset(inputs, targets)\n",
        "# The batch size will be split across GPUs. If you have 2 GPUs,\n",
        "# each will process batch_size / 2 samples.\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T14:35:14.850315Z",
          "iopub.execute_input": "2025-06-18T14:35:14.851088Z",
          "iopub.status.idle": "2025-06-18T14:35:14.924285Z",
          "shell.execute_reply.started": "2025-06-18T14:35:14.851066Z",
          "shell.execute_reply": "2025-06-18T14:35:14.923505Z"
        },
        "id": "113e6748"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "23f49f33",
      "cell_type": "markdown",
      "source": [
        "## 2. Initialize and Wrap the Model\n",
        "Instantiate the model"
      ],
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "23f49f33"
      }
    },
    {
      "id": "86c5fc36",
      "cell_type": "code",
      "source": [
        "model = SimpleModel(input_size, output_size)\n",
        "\n",
        "# IMPORTANT: Wrap the model with nn.DataParallel\n",
        "# This is the key step for data parallelism.\n",
        "# If multiple GPUs are available, this wrapper will handle the data distribution.\n",
        "if num_gpus > 1:\n",
        "    print(f\"Using {num_gpus} GPUs for training!\")\n",
        "    model = nn.DataParallel(model)\n",
        "else:\n",
        "    print(\"Training on a single device (CPU or 1 GPU).\")\n",
        "\n",
        "# Move the model to the primary device. DataParallel will handle the rest.\n",
        "model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T14:36:20.904773Z",
          "iopub.execute_input": "2025-06-18T14:36:20.905451Z",
          "iopub.status.idle": "2025-06-18T14:36:20.913077Z",
          "shell.execute_reply.started": "2025-06-18T14:36:20.905423Z",
          "shell.execute_reply": "2025-06-18T14:36:20.912464Z"
        },
        "id": "86c5fc36",
        "outputId": "4ac456d3-b301-40da-f473-246346a973f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using 2 GPUs for training!\n",
          "output_type": "stream"
        },
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DataParallel(\n  (module): SimpleModel(\n    (fc1): Linear(in_features=784, out_features=128, bias=True)\n    (relu): ReLU()\n    (fc2): Linear(in_features=128, out_features=10, bias=True)\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "577fc2b9",
      "cell_type": "markdown",
      "source": [
        "## 3. Loss and Optimizer"
      ],
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "577fc2b9"
      }
    },
    {
      "id": "95b25c7b",
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T14:36:30.917136Z",
          "iopub.execute_input": "2025-06-18T14:36:30.917424Z",
          "iopub.status.idle": "2025-06-18T14:36:30.921335Z",
          "shell.execute_reply.started": "2025-06-18T14:36:30.917405Z",
          "shell.execute_reply": "2025-06-18T14:36:30.920770Z"
        },
        "id": "95b25c7b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5ceddd76",
      "cell_type": "markdown",
      "source": [
        "## 4. Training Loop"
      ],
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "5ceddd76"
      }
    },
    {
      "id": "96086fd4",
      "cell_type": "code",
      "source": [
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for i, (batch_inputs, batch_targets) in enumerate(data_loader):\n",
        "        # Move data to the primary device. DataParallel will scatter it.\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_targets = batch_targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        # DataParallel automatically splits the batch, sends it to the GPUs,\n",
        "        # executes the forward pass, and gathers the outputs on the primary device.\n",
        "        debug = epoch == 0 and i == 0\n",
        "        outputs = model(batch_inputs, debug=debug)\n",
        "        if debug:\n",
        "            print(\"Outside: input size\", batch_inputs.size(), \"output_size\", outputs.size())\n",
        "        loss = criterion(outputs, batch_targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        # The loss is computed on the primary GPU. The backward pass calculates\n",
        "        # gradients on each GPU, which are then summed on the primary GPU.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "print(\"\\nTraining finished!\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T14:37:56.527863Z",
          "iopub.execute_input": "2025-06-18T14:37:56.528430Z",
          "iopub.status.idle": "2025-06-18T14:38:00.899463Z",
          "shell.execute_reply.started": "2025-06-18T14:37:56.528408Z",
          "shell.execute_reply": "2025-06-18T14:38:00.898449Z"
        },
        "id": "96086fd4",
        "outputId": "ec2d2032-7f21-46ee-c7ab-c913831b14e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nStarting training...\n\tInside the Model: input size torch.Size([128, 784]) output size torch.Size([128, 10])\n\tInside the Model: input size torch.Size([128, 784]) output size torch.Size([128, 10])\nOutside: input size torch.Size([256, 784]) output_size torch.Size([256, 10])\nEpoch [1/20], Loss: 2.3282\nEpoch [2/20], Loss: 2.3176\nEpoch [3/20], Loss: 2.3090\nEpoch [4/20], Loss: 2.3036\nEpoch [5/20], Loss: 2.2976\nEpoch [6/20], Loss: 2.2932\nEpoch [7/20], Loss: 2.2872\nEpoch [8/20], Loss: 2.2796\nEpoch [9/20], Loss: 2.2780\nEpoch [10/20], Loss: 2.2728\nEpoch [11/20], Loss: 2.2654\nEpoch [12/20], Loss: 2.2600\nEpoch [13/20], Loss: 2.2561\nEpoch [14/20], Loss: 2.2523\nEpoch [15/20], Loss: 2.2479\nEpoch [16/20], Loss: 2.2409\nEpoch [17/20], Loss: 2.2359\nEpoch [18/20], Loss: 2.2329\nEpoch [19/20], Loss: 2.2275\nEpoch [20/20], Loss: 2.2236\n\nTraining finished!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "00949f04",
      "cell_type": "markdown",
      "source": [
        "## 5. Accessing the Original Model\n",
        "If you need to save the model's state dict or access the original model\n",
        "without the DataParallel wrapper, you need to use .module"
      ],
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "00949f04"
      }
    },
    {
      "id": "5123ced2",
      "cell_type": "code",
      "source": [
        "if isinstance(model, nn.DataParallel):\n",
        "    original_model = model.module\n",
        "    print(\"\\nModel was wrapped in DataParallel. Accessing the original model via .module\")\n",
        "    torch.save(original_model.state_dict(), 'model_state.pth')\n",
        "else:\n",
        "    original_model = model\n",
        "    print(\"\\nModel was not wrapped. Saving the model directly.\")\n",
        "    torch.save(original_model.state_dict(), 'model_state.pth')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T14:16:32.161780Z",
          "iopub.execute_input": "2025-06-18T14:16:32.162022Z",
          "iopub.status.idle": "2025-06-18T14:16:32.169906Z",
          "shell.execute_reply.started": "2025-06-18T14:16:32.162000Z",
          "shell.execute_reply": "2025-06-18T14:16:32.169166Z"
        },
        "id": "5123ced2",
        "outputId": "1002f67a-79d6-4fe4-cfc6-35c777fd9538"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nModel was wrapped in DataParallel. Accessing the original model via .module\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}