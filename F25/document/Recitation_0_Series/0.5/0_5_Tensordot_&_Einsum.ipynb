{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
        "https://colab.research.google.com/github/CMU-IDeeL/CMU-IDeeL.github.io/blob/master/F25/document/Recitation_0_Series/0.5/0_5_Tensordot_&_Einsum.ipynb)"
      ],
      "metadata": {
        "id": "8Gh-A4ZuOwG0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpo6Llp6jcmo"
      },
      "source": [
        " # 0.5 Tensordot & Einsum\n",
        "\n",
        " ## Authors:\n",
        "\n",
        "\n",
        "*   Ahmed Alhassan\n",
        "*   Michael Kireeff\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " This notebook explores `einsum` (Einstein summation) and `tensordot`, two powerful functions for tensor operations. They offer a concise way to do dot products, outer products, transpositions, matrix math, and more complex tensor contractions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj_g9RWOjcms"
      },
      "source": [
        " ## Mathematical Foundation\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is a Tensor?\n",
        "\n",
        "Tensors are multi-dimensional arrays capable of representing data from scalars (0D) to vectors (1D), matrices (2D), and higher-dimensional structures. Both PyTorch and Numpy provide a wide range of tensor operations."
      ],
      "metadata": {
        "id": "DBXSl-2kNOGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor Operations\n",
        "\n",
        "It is the set of mathematical operations that involve the use of tensors, those operations include:\n",
        "\n",
        "*   Mathematical transformations: Addition, multiplication, and advanced matrix computations\n",
        "*   Shape manipulation: Reshaping, stacking, and squeezing dimensions\n",
        "*   Data extraction: Indexing, slicing, and conditional filtering\n",
        "*   Reduction operations: Summing, averaging, and finding max/min values\n",
        "*   GPU-accelerated processing: Harnessing parallel computing power\n",
        "*   Automatic differentiation: Calculating gradients for machine learning"
      ],
      "metadata": {
        "id": "AtoFabOaOdla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Einstein Summation Convention\n",
        "\n",
        " The Einstein summation convention is a notational convention that implies summation over repeated indices.\n",
        "\n",
        " For example, if we have tensors $A_{ij}$ and $B_{jk}$, then $C_{ik} = A_{ij} B_{jk}$ represents:\n",
        "\n",
        "\n",
        "\n",
        " $C_{ik} = \\sum_j A_{ij} B_{jk}$\n",
        "\n",
        "\n",
        "\n",
        " This convention eliminates the need to explicitly write summation symbols, making tensor operations more concise.\n",
        "\n",
        "\n",
        "\n",
        " ### Tensordot (Tensor Contraction)\n",
        "\n",
        " Tensor contraction is a generalization of matrix multiplication to higher-dimensional arrays.\n",
        "\n",
        " When we contract along certain dimensions, we perform element-wise multiplication followed by summation\n",
        "\n",
        " along those dimensions."
      ],
      "metadata": {
        "id": "M9hHg6SpNMnc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5AEYUQajcms"
      },
      "source": [
        " ## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXQvrVvjjcmt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7ZuDRITjcmu"
      },
      "source": [
        " ## Einsum\n",
        "\n",
        "\n",
        "\n",
        " **Einstein summation** (`einsum`) is a super handy function in NumPy and PyTorch. It uses a special string notation (Einstein's convention) to perform a wide variety of tensor operations. ðŸ’¡\n",
        "\n",
        "\n",
        "\n",
        " The basic idea is `einsum('input_specs -> output_spec', tensor1, tensor2, ...)`.\n",
        "\n",
        " - Indices repeated in inputs but not in the output get summed out (contracted).\n",
        "\n",
        " - Indices appearing in an input but not the output are also summed out.\n",
        "\n",
        " - The order of indices in the `output_spec` defines the final tensor's axis order.\n",
        "\n",
        " - If `-> output_spec` is missing, `einsum` infers it from unsummed input indices, ordered alphabetically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajmQeW-5jcmu"
      },
      "source": [
        " ```numpy.einsum(subscripts, *operands, out=None, dtype=None, order='K', casting='safe')```\n",
        "\n",
        "\n",
        "\n",
        " ```torch.einsum(equation, *operands)```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD95IUIcjcmv"
      },
      "source": [
        " ### Vector Operations\n",
        "\n",
        "\n",
        "\n",
        " Let's start with some basic vector ops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfNETAV2jcmv",
        "outputId": "fa20fdf9-0a4d-4dac-8c09-1af6dc9e7d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "u_vec_ops: [1 2 3] Shape: (3,)\n",
            "v_vec_ops: [4 5 6] Shape: (3,)\n"
          ]
        }
      ],
      "source": [
        "u_vec_ops = np.array([1, 2, 3])\n",
        "v_vec_ops = np.array([4, 5, 6])\n",
        "print(\"u_vec_ops:\", u_vec_ops, \"Shape:\", u_vec_ops.shape)\n",
        "print(\"v_vec_ops:\", v_vec_ops, \"Shape:\", v_vec_ops.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRoSOh8tjcmw"
      },
      "source": [
        " #### Sum of Elements (Vector Sum Reduction)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This operation calculates the sum of all elements in a vector.\n",
        "\n",
        " Equation: $s = \\sum_i u_i$.\n",
        "\n",
        "\n",
        "\n",
        " For `einsum`:\n",
        "\n",
        " -  `'i->'`:\n",
        "\n",
        "   -  `i`: A single 1D input indexed by `i`.\n",
        "\n",
        "   -  `->`: No output index, so `i` is summed over."
      ],
      "metadata": {
        "id": "KyqUM3WFKN7g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cl0rl1Bvjcmw",
        "outputId": "93b18268-db80-44f7-dee9-0a430f7c9337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of elements in u_vec_ops (einsum): 6\n",
            "Expected (np.sum): 6\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "sum_elements_einsum = np.einsum('i->', u_vec_ops)\n",
        "sum_elements_expected = np.sum(u_vec_ops)\n",
        "print(\"Sum of elements in u_vec_ops (einsum):\", sum_elements_einsum)\n",
        "print(\"Expected (np.sum):\", sum_elements_expected)\n",
        "print(\"Results match:\", np.allclose(sum_elements_einsum, sum_elements_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1jU9Nrpjcmv"
      },
      "source": [
        " #### Element-wise Vector Product (Hadamard Product)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " This is the **Hadamard product**, meaning element-by-element multiplication.\n",
        "\n",
        " If $w = u \\odot v$, then each output element is $w_i = u_i \\cdot v_i$.\n",
        "\n",
        "\n",
        "\n",
        " For `einsum`'s `subscripts`:\n",
        "\n",
        " -  `'i,i->i'`:\n",
        "\n",
        "   -  `i,i`: Two 1D inputs, both indexed by `i`.\n",
        "\n",
        "   -  `->i`: Output is 1D, also indexed by `i`.\n",
        "\n",
        "   -  Since `i` is in both inputs and the output, it's an element-wise operation. No summation occurs over `i`."
      ],
      "metadata": {
        "id": "ZNnAiTtSKgKH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SurEHSrRjcmv",
        "outputId": "6d7fbe33-2c12-4ef2-ffde-6df124a51456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result (einsum): [ 4 10 18]\n",
            "Expected (np.multiply): [ 4 10 18]\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "hadamard_einsum = np.einsum('i,i->i', u_vec_ops, v_vec_ops)\n",
        "hadamard_expected = np.multiply(u_vec_ops, v_vec_ops)\n",
        "print(\"Result (einsum):\", hadamard_einsum)\n",
        "print(\"Expected (np.multiply):\", hadamard_expected)\n",
        "print(\"Results match:\", np.allclose(hadamard_einsum, hadamard_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8lCfduKjcmw"
      },
      "source": [
        " #### Dot Product (Inner Product)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " The **dot product** (or scalar product) sums the products of corresponding vector elements, yielding a single scalar value.\n",
        "\n",
        " The equation is $s = u \\cdot v = \\sum_i u_i v_i$.\n",
        "\n",
        "\n",
        "\n",
        " For `einsum`:\n",
        "\n",
        " -  `'i,i->'`:\n",
        "\n",
        "   -  `i,i`: Two 1D inputs, indexed by `i`.\n",
        "\n",
        "   -  `->`: No output index means `i` (which is common to inputs but not output) is summed over.\n",
        "\n",
        " -  `'i,i'`: This is shorthand; if `->` is omitted, repeated indices not in an explicit output are summed."
      ],
      "metadata": {
        "id": "Avhn4TyIKlf9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be8cV3Y3jcmw",
        "outputId": "86e316eb-6e99-49ed-d3d3-380e7e794ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result (explicit output 'i,i->'): 32\n",
            "Result (implicit sum 'i,i'): 32\n",
            "Expected (np.dot): 32\n",
            "All results match: True\n",
            "\n",
            "Performance Comparison:\n",
            "Einsum timing for dot product:\n",
            "2.07 Âµs Â± 287 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)\n",
            "NumPy dot timing for dot product:\n",
            "861 ns Â± 15.9 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each)\n"
          ]
        }
      ],
      "source": [
        "dot_explicit = np.einsum('i,i->', u_vec_ops, v_vec_ops)\n",
        "dot_implicit = np.einsum('i,i', u_vec_ops, v_vec_ops)\n",
        "dot_expected = np.dot(u_vec_ops, v_vec_ops)\n",
        "\n",
        "print(\"Result (explicit output 'i,i->'):\", dot_explicit)\n",
        "print(\"Result (implicit sum 'i,i'):\", dot_implicit)\n",
        "print(\"Expected (np.dot):\", dot_expected)\n",
        "print(\"All results match:\", np.allclose([dot_explicit, dot_implicit, dot_expected], [dot_expected]*3))\n",
        "\n",
        "# Performance comparison (actual %timeit commented out for brevity in output)\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(\"Einsum timing for dot product:\")\n",
        "%timeit np.einsum('i,i->', u_vec_ops, v_vec_ops)\n",
        "print(\"NumPy dot timing for dot product:\")\n",
        "%timeit np.dot(u_vec_ops, v_vec_ops)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtOM-Ni4jcmw"
      },
      "source": [
        " #### Outer Product\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **outer product** of two vectors $u$ (size $m$) and $v$ (size $n$) results in an $m \\times n$ matrix $M$.\n",
        "\n",
        " Each element $M_{ij} = u_i v_j$.\n",
        "\n",
        "\n",
        "\n",
        " For `einsum`:\n",
        "\n",
        " -  `'i,j->ij'`:\n",
        "\n",
        "   -  `i,j`: First input indexed by `i`, second by `j`.\n",
        "\n",
        "   -  `->ij`: Output is 2D, with axes corresponding to `i` and `j`. No summation as `i` and `j` are distinct and present in the output."
      ],
      "metadata": {
        "id": "keujOT8aKqLX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1A0UMIRjcmw",
        "outputId": "5bb04d64-a2bd-459c-ff53-155985c4c7e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outer product (einsum):\n",
            " [[ 4  5  6]\n",
            " [ 8 10 12]\n",
            " [12 15 18]]\n",
            "Expected (np.outer):\n",
            " [[ 4  5  6]\n",
            " [ 8 10 12]\n",
            " [12 15 18]]\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "outer_einsum = np.einsum('i,j->ij', u_vec_ops, v_vec_ops)\n",
        "outer_expected = np.outer(u_vec_ops, v_vec_ops)\n",
        "print(\"Outer product (einsum):\\n\", outer_einsum)\n",
        "print(\"Expected (np.outer):\\n\", outer_expected)\n",
        "print(\"Results match:\", np.allclose(outer_einsum, outer_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfokNqmbjcmw"
      },
      "source": [
        " ### Matrix Operations\n",
        "\n",
        "\n",
        "\n",
        " Now, let's work with matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grUra1TMjcmw",
        "outputId": "0dcf6f55-f072-458c-8200-722071fe4fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A_mat (2x3):\n",
            " [[1 2 3]\n",
            " [4 5 6]]\n",
            "B_mat (3x2):\n",
            " [[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n",
            "C_mat (2x2):\n",
            " [[1 2]\n",
            " [3 4]]\n",
            "u_vec_ops (3,):\n",
            " [1 2 3]\n",
            "w_matvec (2,):\n",
            " [7 8]\n"
          ]
        }
      ],
      "source": [
        "A_mat = np.array([[1, 2, 3], [4, 5, 6]]) # Shape (2,3)\n",
        "B_mat = np.array([[1, 2], [3, 4], [5, 6]]) # Shape (3,2)\n",
        "C_mat = np.array([[1, 2], [3, 4]])     # Shape (2,2)\n",
        "# u_vec_ops is (3,) - can be used as u_matvec\n",
        "# w_matvec needs to be defined if used for vector-matrix product with C_mat, or matrix-matrix-vector product\n",
        "w_matvec = np.array([7, 8]) # Shape (2,) for (A_mat @ B_mat) @ w_matvec or w_matvec @ C_mat\n",
        "\n",
        "print(\"A_mat (2x3):\\n\", A_mat)\n",
        "print(\"B_mat (3x2):\\n\", B_mat)\n",
        "print(\"C_mat (2x2):\\n\", C_mat)\n",
        "print(\"u_vec_ops (3,):\\n\", u_vec_ops) # Reusing u_vec_ops for matrix-vector product\n",
        "print(\"w_matvec (2,):\\n\", w_matvec)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm11AvR1jcmw"
      },
      "source": [
        " #### Matrix Transpose\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **transpose** of a matrix $M$ (denoted $M^T$) swaps its rows and columns. If $M$ is $m \\times n$, $M^T$ is $n \\times m$.\n",
        "\n",
        " Equation: $(M^T)_{ji} = M_{ij}$.\n",
        "\n",
        "\n",
        "\n",
        " For `einsum`:\n",
        "\n",
        " -  `'ij->ji'`:\n",
        "\n",
        "   -  `ij`: Input is 2D (rows `i`, columns `j`).\n",
        "\n",
        "   -  `->ji`: Output is 2D, with original rows `i` now columns, and original columns `j` now rows."
      ],
      "metadata": {
        "id": "1FVRWIzsKu5f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCzpblxfjcmx",
        "outputId": "6f32c4c8-4591-49dd-ce06-16fa8f5133b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transpose of A_mat (einsum):\n",
            " [[1 4]\n",
            " [2 5]\n",
            " [3 6]]\n",
            "Expected (A_mat.T):\n",
            " [[1 4]\n",
            " [2 5]\n",
            " [3 6]]\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "transpose_einsum = np.einsum('ij->ji', A_mat)\n",
        "transpose_expected = A_mat.T\n",
        "print(\"Transpose of A_mat (einsum):\\n\", transpose_einsum)\n",
        "print(\"Expected (A_mat.T):\\n\", transpose_expected)\n",
        "print(\"Results match:\", np.allclose(transpose_einsum, transpose_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1SLZ5HEjcmx"
      },
      "source": [
        " #### Trace of a Matrix\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **trace** of a square matrix $M$ (denoted $\\text{tr}(M)$) is the sum of elements on its main diagonal.\n",
        "\n",
        " Equation: $\\text{tr}(M) = \\sum_i M_{ii}$.\n",
        "\n",
        "\n",
        "\n",
        " For `einsum`:\n",
        "\n",
        " -  `'ii->'`:\n",
        "\n",
        "   -  `ii`: Input is 2D. Using the same index `i` for rows and columns selects diagonal elements.\n",
        "\n",
        "   -  `->`: No output index, so these diagonal elements are summed."
      ],
      "metadata": {
        "id": "NjLGj76nKyXm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfO88YR5jcmx",
        "outputId": "2606744f-490f-4d39-8abe-c5cbd036f08b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trace of C_mat (einsum 'ii->'): 5\n",
            "Expected (np.trace): 5\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "trace_einsum = np.einsum('ii->', C_mat) # C_mat is 2x2\n",
        "trace_expected = np.trace(C_mat)\n",
        "print(\"Trace of C_mat (einsum 'ii->'):\", trace_einsum)\n",
        "print(\"Expected (np.trace):\", trace_expected)\n",
        "print(\"Results match:\", np.allclose(trace_einsum, trace_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJYaNsP0jcmy"
      },
      "source": [
        " #### Diagonal Extraction\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracts the main diagonal of a square matrix $M$ into a vector.\n",
        "\n",
        " Equation: $d_i = M_{ii}$.\n",
        "\n",
        "\n",
        "\n",
        " For `einsum`:\n",
        "\n",
        " -  `'ii->i'`:\n",
        "\n",
        "   -  `ii`: Selects diagonal elements.\n",
        "\n",
        "   -  `->i`: Output is 1D, containing these elements."
      ],
      "metadata": {
        "id": "H_zUMh87K2wX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5aEj90xjcmy",
        "outputId": "b4238d3f-ab1b-4785-f021-d9d5b0935869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diagonal of C_mat (einsum 'ii->i'): [1 4]\n",
            "Expected (np.diag): [1 4]\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "diag_extract_einsum = np.einsum('ii->i', C_mat)\n",
        "diag_extract_expected = np.diag(C_mat)\n",
        "print(\"Diagonal of C_mat (einsum 'ii->i'):\", diag_extract_einsum)\n",
        "print(\"Expected (np.diag):\", diag_extract_expected)\n",
        "print(\"Results match:\", np.allclose(diag_extract_einsum, diag_extract_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60G3mliAjcmy"
      },
      "source": [
        " #### Sum along an Axis\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Sum rows (collapse columns, sum over `j` for each `i`):** For $M_{ij}$, $r_i = \\sum_j M_{ij}$.\n",
        "\n",
        "   -  `'ij->i'`: Index `j` (columns) is summed out.\n",
        "\n",
        " 2. **Sum columns (collapse rows, sum over `i` for each `j`):** For $M_{ij}$, $c_j = \\sum_i M_{ij}$.\n",
        "\n",
        "   -  `'ij->j'`: Index `i` (rows) is summed out."
      ],
      "metadata": {
        "id": "peGB1NzxK6G0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JJFfdiOjcmy",
        "outputId": "32080a71-5561-422f-b5db-e4b5451f555e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A_mat:\n",
            " [[1 2 3]\n",
            " [4 5 6]]\n",
            "Sum rows of A_mat (sum over columns for each row, einsum 'ij->i'): [ 6 15]\n",
            "Expected (np.sum(A_mat, axis=1)): [ 6 15]\n",
            "Results match: True\n",
            "\n",
            "Sum columns of A_mat (sum over rows for each column, einsum 'ij->j'): [5 7 9]\n",
            "Expected (np.sum(A_mat, axis=0)): [5 7 9]\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "print(\"Matrix A_mat:\\n\", A_mat)\n",
        "sum_along_axis1_einsum = np.einsum('ij->i', A_mat) # Sum over columns for each row\n",
        "sum_along_axis1_expected = np.sum(A_mat, axis=1)\n",
        "print(\"Sum rows of A_mat (sum over columns for each row, einsum 'ij->i'):\", sum_along_axis1_einsum)\n",
        "print(\"Expected (np.sum(A_mat, axis=1)):\", sum_along_axis1_expected)\n",
        "print(\"Results match:\", np.allclose(sum_along_axis1_einsum, sum_along_axis1_expected))\n",
        "\n",
        "sum_along_axis0_einsum = np.einsum('ij->j', A_mat) # Sum over rows for each column\n",
        "sum_along_axis0_expected = np.sum(A_mat, axis=0)\n",
        "print(\"\\nSum columns of A_mat (sum over rows for each column, einsum 'ij->j'):\", sum_along_axis0_einsum)\n",
        "print(\"Expected (np.sum(A_mat, axis=0)):\", sum_along_axis0_expected)\n",
        "print(\"Results match:\", np.allclose(sum_along_axis0_einsum, sum_along_axis0_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG08BCWRjcmy"
      },
      "source": [
        " #### Frobenius Norm (Matrix Norm)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Mathematical notation: $||A||_F = \\sqrt{\\sum_i \\sum_j A_{ij}^2}$\n",
        "\n",
        " Einsum can compute the squared norm: $\\sum_i \\sum_j A_{ij}^2 = \\sum_i \\sum_j A_{ij} A_{ij}$\n",
        "\n",
        " Einsum notation: `'ij,ij->'` for the squared norm."
      ],
      "metadata": {
        "id": "ZOxRxh_TK9iz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_oR1c_zjcmy",
        "outputId": "a1427c46-bec3-425f-e2df-cb55a4ce4cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A_mat for Frobenius norm:\n",
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "Squared Frobenius norm (einsum 'ij,ij->'): 91\n",
            "Frobenius norm (einsum, sqrt): 9.539392014169456\n",
            "Expected (np.linalg.norm): 9.539392014169456\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "# Using A_mat (2x3) for Frobenius norm\n",
        "frobenius_sq_einsum = np.einsum('ij,ij->', A_mat, A_mat)\n",
        "frobenius_einsum = np.sqrt(frobenius_sq_einsum)\n",
        "frobenius_expected = np.linalg.norm(A_mat, 'fro')\n",
        "\n",
        "print(f\"A_mat for Frobenius norm:\\n{A_mat}\")\n",
        "print(\"Squared Frobenius norm (einsum 'ij,ij->'):\", frobenius_sq_einsum)\n",
        "print(\"Frobenius norm (einsum, sqrt):\", frobenius_einsum)\n",
        "print(\"Expected (np.linalg.norm):\", frobenius_expected)\n",
        "print(\"Results match:\", np.allclose(frobenius_einsum, frobenius_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-XGGOkDjcmy"
      },
      "source": [
        " #### Element-wise Matrix Product (Hadamard)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For two matrices $M_1, M_2$ of same dimensions, $P = M_1 \\odot M_2 \\implies P_{ij} = (M_1)_{ij} (M_2)_{ij}$.\n",
        "\n",
        " -  `'ij,ij->ij'`: Common indices `i,j` in inputs and output means element-wise."
      ],
      "metadata": {
        "id": "Chao8TbJLDPA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXu2ZQqhjcmy",
        "outputId": "3bae4f77-af5b-471a-8ae9-4992f2f10309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element-wise product C_mat * D_mat (einsum 'ij,ij->ij'):\n",
            " [[ 5 12]\n",
            " [21 32]]\n",
            "Expected (C_mat * D_mat):\n",
            " [[ 5 12]\n",
            " [21 32]]\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "D_mat = np.array([[5, 6], [7, 8]]) # Shape (2,2), same as C_mat\n",
        "hadamard_mat_einsum = np.einsum('ij,ij->ij', C_mat, D_mat)\n",
        "hadamard_mat_expected = C_mat * D_mat # or np.multiply(C_mat, D_mat)\n",
        "print(\"Element-wise product C_mat * D_mat (einsum 'ij,ij->ij'):\\n\", hadamard_mat_einsum)\n",
        "print(\"Expected (C_mat * D_mat):\\n\", hadamard_mat_expected)\n",
        "print(\"Results match:\", np.allclose(hadamard_mat_einsum, hadamard_mat_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6I-59Wqjcmx"
      },
      "source": [
        " #### Matrix-Vector Product\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Product of an $m \\times n$ matrix $M$ and an $n \\times 1$ (column) vector $u$, resulting in an $m \\times 1$ vector $y$.\n",
        "\n",
        " Equation: $y = M u \\implies y_i = \\sum_j M_{ij} u_j$.\n",
        "\n",
        "\n",
        "\n",
        " For `einsum`:\n",
        "\n",
        " -  `'ij,j->i'`:\n",
        "\n",
        "   -  `ij,j`: Matrix $M$ (indices `i,j`) and vector $u$ (index `j`).\n",
        "\n",
        "   -  `->i`: Output is 1D (index `i`).\n",
        "\n",
        "   -  Index `j` is common to inputs but not output, so it's summed. Index `i` (rows of $M$) is preserved."
      ],
      "metadata": {
        "id": "1-yRzNrTLHRN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20AI2hTzjcmx",
        "outputId": "235bcbfe-2784-495a-9142-9189f8c60a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix-vector product A_mat @ u_vec_ops (einsum):\n",
            " [14 32]\n",
            "Expected (np.dot):\n",
            " [14 32]\n",
            "Results match: True\n",
            "\n",
            "Step-by-step calculation for A_mat @ u_vec_ops:\n",
            "A_mat[0,:] ([1 2 3]) @ u_vec_ops ([1 2 3]) = 14\n",
            "A_mat[1,:] ([4 5 6]) @ u_vec_ops ([1 2 3]) = 32\n"
          ]
        }
      ],
      "source": [
        "matvec_einsum = np.einsum('ij,j->i', A_mat, u_vec_ops) # Using u_vec_ops as the vector\n",
        "matvec_expected = np.dot(A_mat, u_vec_ops) # or A_mat @ u_vec_ops\n",
        "print(\"Matrix-vector product A_mat @ u_vec_ops (einsum):\\n\", matvec_einsum)\n",
        "print(\"Expected (np.dot):\\n\", matvec_expected)\n",
        "print(\"Results match:\", np.allclose(matvec_einsum, matvec_expected))\n",
        "\n",
        "# Step-by-step calculation for understanding:\n",
        "print(\"\\nStep-by-step calculation for A_mat @ u_vec_ops:\")\n",
        "print(f\"A_mat[0,:] ({A_mat[0,:]}) @ u_vec_ops ({u_vec_ops}) = {np.dot(A_mat[0,:], u_vec_ops)}\")\n",
        "print(f\"A_mat[1,:] ({A_mat[1,:]}) @ u_vec_ops ({u_vec_ops}) = {np.dot(A_mat[1,:], u_vec_ops)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD2jqrxQjcmx"
      },
      "source": [
        " #### Vector-Matrix Product\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Product of a $1 \\times m$ (row) vector $w^T$ and an $m \\times n$ matrix $M$, resulting in a $1 \\times n$ vector $z^T$.\n",
        "\n",
        " (Represent $w^T$ as a 1D array `w_arr`).\n",
        "\n",
        " Equation: $z^T = w^T M \\implies z_j = \\sum_i w_i M_{ij}$.\n",
        "\n",
        "\n",
        "\n",
        " For `einsum`:\n",
        "\n",
        " -  `'i,ij->j'`:\n",
        "\n",
        "   -  `i,ij`: Vector `w_arr` (index `i`) and matrix $M$ (indices `i,j`).\n",
        "\n",
        "   -  `->j`: Output is 1D (index `j`).\n",
        "\n",
        "   -  Index `i` is summed. Index `j` (columns of $M$) is preserved."
      ],
      "metadata": {
        "id": "d4jl6y7ILLHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XueoJ_hejcmx",
        "outputId": "e7e0692e-bf3d-4573-872f-b771821c2d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector-matrix product w_matvec @ C_mat (einsum):\n",
            " [31 46]\n",
            "Expected (np.dot):\n",
            " [31 46]\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "# Using w_matvec (shape 2) and C_mat (shape 2x2)\n",
        "vecmat_einsum = np.einsum('i,ij->j', w_matvec, C_mat)\n",
        "vecmat_expected = np.dot(w_matvec, C_mat) # or w_matvec @ C_mat\n",
        "print(\"Vector-matrix product w_matvec @ C_mat (einsum):\\n\", vecmat_einsum)\n",
        "print(\"Expected (np.dot):\\n\", vecmat_expected)\n",
        "print(\"Results match:\", np.allclose(vecmat_einsum, vecmat_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udjad58bjcmx"
      },
      "source": [
        " #### Matrix Multiplication\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Product of an $m \\times n$ matrix $M_1$ and an $n \\times p$ matrix $M_2$, results in an $m \\times p$ matrix $P$.\n",
        "\n",
        " Equation: $P = M_1 M_2 \\implies P_{ik} = \\sum_j (M_1)_{ij} (M_2)_{jk}$.\n",
        "\n",
        "\n",
        "\n",
        " For `einsum`:\n",
        "\n",
        " -  `'ij,jk->ik'`:\n",
        "\n",
        "   -  `ij,jk`: Matrix $M_1$ (`i,j`) and $M_2$ (`j,k`).\n",
        "\n",
        "   -  `->ik`: Output is 2D (`i,k`).\n",
        "\n",
        "   -  `j` is summed. `i` (rows of $M_1$) and `k` (columns of $M_2$) define output shape.\n",
        "\n",
        " -  `'ij,jk'`: Shorthand, `ik` output is inferred."
      ],
      "metadata": {
        "id": "b1rpXZqQLOTa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGuRtcqMjcmx",
        "outputId": "f50cc73d-fa94-4afd-aaa8-565e11791eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix multiplication A_mat @ B_mat (einsum, explicit 'ij,jk->ik'):\n",
            " [[22 28]\n",
            " [49 64]]\n",
            "Matrix multiplication A_mat @ B_mat (einsum, implicit 'ij,jk'):\n",
            " [[22 28]\n",
            " [49 64]]\n",
            "Expected (np.matmul):\n",
            " [[22 28]\n",
            " [49 64]]\n",
            "All results match: True\n"
          ]
        }
      ],
      "source": [
        "matmul_einsum_explicit = np.einsum('ij,jk->ik', A_mat, B_mat)\n",
        "matmul_einsum_implicit = np.einsum('ij,jk', A_mat, B_mat)\n",
        "matmul_expected = np.matmul(A_mat, B_mat) # or A_mat @ B_mat\n",
        "print(\"Matrix multiplication A_mat @ B_mat (einsum, explicit 'ij,jk->ik'):\\n\", matmul_einsum_explicit)\n",
        "print(\"Matrix multiplication A_mat @ B_mat (einsum, implicit 'ij,jk'):\\n\", matmul_einsum_implicit)\n",
        "print(\"Expected (np.matmul):\\n\", matmul_expected)\n",
        "print(\"All results match:\", np.allclose(matmul_einsum_explicit, matmul_expected) and np.allclose(matmul_einsum_implicit, matmul_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtY51XDtjcmx"
      },
      "source": [
        " #### Matrix Multiplication with Reduction\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performs $M_1 M_2 = P$, then sums $P$ over one or both axes.\n",
        "\n",
        " 1. **Sum columns of $P$ (sum each row of $P$):** $r_i = \\sum_k P_{ik} = \\sum_k \\sum_j (M_1)_{ij} (M_2)_{jk}$.\n",
        "\n",
        "   -  `'ij,jk->i'`: Index `k` from $(M_2)_{jk}$ is summed out.\n",
        "\n",
        " 2. **Sum rows of $P$ (sum each column of $P$):** $s_k = \\sum_i P_{ik} = \\sum_i \\sum_j (M_1)_{ij} (M_2)_{jk}$.\n",
        "\n",
        "   -  `'ij,jk->k'`: Index `i` from $(M_1)_{ij}$ is summed out.\n",
        "\n",
        " 3. **Sum all elements of $P$:** $t = \\sum_i \\sum_k P_{ik}$.\n",
        "\n",
        "   -  `'ij,jk->'`: Both `i` and `k` are summed out."
      ],
      "metadata": {
        "id": "F8ATZVSiLUHY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l26tgyOIjcmx",
        "outputId": "cc5846d8-370e-4ef3-aeb5-2eb1140ecab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A_mat @ B_mat then sum over columns (einsum 'ij,jk->i'):\n",
            " [ 50 113]\n",
            "Expected (sum rows of P_product):\n",
            " [ 50 113]\n",
            "Results match: True\n",
            "\n",
            "A_mat @ B_mat then sum over rows (einsum 'ij,jk->k'):\n",
            " [71 92]\n",
            "Expected (sum cols of P_product):\n",
            " [71 92]\n",
            "Results match: True\n",
            "\n",
            "A_mat @ B_mat then sum all elements (einsum 'ij,jk->'):\n",
            " 163\n",
            "Expected (sum all elements of P_product):\n",
            " 163\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "P_product = A_mat @ B_mat # Result is (2x2)\n",
        "\n",
        "# Sum over columns of P_product (sum each row)\n",
        "sum_cols_einsum = np.einsum('ij,jk->i', A_mat, B_mat)\n",
        "sum_cols_expected = np.sum(P_product, axis=1)\n",
        "print(\"A_mat @ B_mat then sum over columns (einsum 'ij,jk->i'):\\n\", sum_cols_einsum)\n",
        "print(\"Expected (sum rows of P_product):\\n\", sum_cols_expected)\n",
        "print(\"Results match:\", np.allclose(sum_cols_einsum, sum_cols_expected))\n",
        "\n",
        "# Sum over rows of P_product (sum each column)\n",
        "sum_rows_einsum = np.einsum('ij,jk->k', A_mat, B_mat)\n",
        "sum_rows_expected = np.sum(P_product, axis=0)\n",
        "print(\"\\nA_mat @ B_mat then sum over rows (einsum 'ij,jk->k'):\\n\", sum_rows_einsum)\n",
        "print(\"Expected (sum cols of P_product):\\n\", sum_rows_expected)\n",
        "print(\"Results match:\", np.allclose(sum_rows_einsum, sum_rows_expected))\n",
        "\n",
        "# Sum all elements of P_product\n",
        "sum_all_einsum = np.einsum('ij,jk->', A_mat, B_mat)\n",
        "sum_all_expected = np.sum(P_product)\n",
        "print(\"\\nA_mat @ B_mat then sum all elements (einsum 'ij,jk->'):\\n\", sum_all_einsum)\n",
        "print(\"Expected (sum all elements of P_product):\\n\", sum_all_expected)\n",
        "print(\"Results match:\", np.allclose(sum_all_einsum, sum_all_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vg_Kb3djcmy"
      },
      "source": [
        " #### Combination of Operations (Chained Operations)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " `einsum` can chain multiple operations, e.g., $(M_1 M_2)w$.\n",
        "\n",
        "\n",
        "\n",
        " 1. **$(M_1 M_2)w \\rightarrow \\text{vector}$**:\n",
        "\n",
        "   $M_1 (m \\times n), M_2 (n \\times p), w (p \\times 1)$.\n",
        "\n",
        "   Equation: $r_i = \\sum_k \\left( \\sum_j (M_1)_{ij} (M_2)_{jk} \\right) w_k$.\n",
        "\n",
        "   -  `'ij,jk,k->i'`: `j` and `k` are summed out.\n",
        "\n",
        "\n",
        "\n",
        " 2. **$\\text{sum}((M_1 M_2)w) \\rightarrow \\text{scalar}$**:\n",
        "\n",
        "   Equation: $s = \\sum_i \\sum_k \\left( \\sum_j (M_1)_{ij} (M_2)_{jk} \\right) w_k$.\n",
        "\n",
        "   -  `'ij,jk,k->'`: `i, j, k` are summed out."
      ],
      "metadata": {
        "id": "6IgY9L0-LY3W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns-o1Jnrjcm2",
        "outputId": "5e5278d6-e6c7-4903-d888-36c6a6338dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result of (A_mat B_mat) w_matvec (einsum 'ij,jk,k->i'): [378 855]\n",
            "Expected ((A_mat@B_mat) @ w_matvec):\n",
            " [378 855]\n",
            "Results match: True\n",
            "\n",
            "Result of sum((A_mat B_mat) w_matvec) (einsum 'ij,jk,k->'): 1233\n",
            "Expected (sum of ((A_mat@B_mat) @ w_matvec)):\n",
            " 1233\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "# A_mat (2,3), B_mat (3,2), w_matvec (2,)\n",
        "# (A_mat @ B_mat) is (2,2). (A_mat @ B_mat) @ w_matvec is (2,)\n",
        "chain1_einsum = np.einsum('ij,jk,k->i', A_mat, B_mat, w_matvec)\n",
        "chain1_expected = (A_mat @ B_mat) @ w_matvec\n",
        "print(\"Result of (A_mat B_mat) w_matvec (einsum 'ij,jk,k->i'):\", chain1_einsum)\n",
        "print(\"Expected ((A_mat@B_mat) @ w_matvec):\\n\", chain1_expected)\n",
        "print(\"Results match:\", np.allclose(chain1_einsum, chain1_expected))\n",
        "\n",
        "chain2_einsum = np.einsum('ij,jk,k->', A_mat, B_mat, w_matvec)\n",
        "chain2_expected = np.sum((A_mat @ B_mat) @ w_matvec)\n",
        "print(\"\\nResult of sum((A_mat B_mat) w_matvec) (einsum 'ij,jk,k->'):\", chain2_einsum)\n",
        "print(\"Expected (sum of ((A_mat@B_mat) @ w_matvec)):\\n\", chain2_expected)\n",
        "print(\"Results match:\", np.allclose(chain2_einsum, chain2_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C95UGXJjcm2"
      },
      "source": [
        " 3. **$M_1 M_2 M_3 \\rightarrow \\text{matrix}$**:\n",
        "\n",
        "   $M_1 (m \\times n), M_2 (n \\times p), M_3 (p \\times q)$.\n",
        "\n",
        "   Equation: $R_{il} = \\sum_k \\left( \\sum_j (M_1)_{ij} (M_2)_{jk} \\right) (M_3)_{kl}$.\n",
        "\n",
        "   -  `'ij,jk,kl->il'`: `j` and `k` are summed out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QBhfxASjcm2",
        "outputId": "b7a4051f-e53f-4bf6-dd2e-9e7ebd3faafa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Result of A_mat B_mat C_mat (A_mat@B_mat@C_mat) (einsum 'ij,jk,kl->il'):\n",
            " [[106 156]\n",
            " [241 354]]\n",
            "Expected (A_mat@B_mat@C_mat):\n",
            " [[106 156]\n",
            " [241 354]]\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "# A_mat (2,3), B_mat (3,2), C_mat (2,2). (A_mat@B_mat@C_mat) is (2,2).\n",
        "chain3_einsum = np.einsum('ij,jk,kl->il', A_mat, B_mat, C_mat)\n",
        "chain3_expected = A_mat @ B_mat @ C_mat\n",
        "print(\"\\nResult of A_mat B_mat C_mat (A_mat@B_mat@C_mat) (einsum 'ij,jk,kl->il'):\\n\", chain3_einsum)\n",
        "print(\"Expected (A_mat@B_mat@C_mat):\\n\", chain3_expected)\n",
        "print(\"Results match:\", np.allclose(chain3_einsum, chain3_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZONRH2Gjcm2"
      },
      "source": [
        " ### Tensor Operations with Einsum\n",
        "\n",
        "\n",
        "\n",
        " `einsum` really shines with tensors of rank 3+."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEjdqMRrjcm3",
        "outputId": "76cc8ee9-55d6-407e-d285-867bc074eebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of Tensor_A_orig: (2, 3, 4, 5)\n",
            "Dimensions of Tensor_B_orig: (2, 3, 5, 6)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42) # For reproducibility\n",
        "Tensor_A_orig = np.random.rand(2, 3, 4, 5) # b, c, h, w for example\n",
        "Tensor_B_orig = np.random.rand(2, 3, 5, 6) # b, c, w, d for example\n",
        "print(\"Dimensions of Tensor_A_orig:\", Tensor_A_orig.shape)\n",
        "print(\"Dimensions of Tensor_B_orig:\", Tensor_B_orig.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy9csOnCjcm3"
      },
      "source": [
        " #### \"...\" Operator (Ellipsis)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ellipsis (`...`) is a placeholder for any number of leading dimensions. It's great for batch operations without naming all leading dimensions.\n",
        "\n",
        "\n",
        "\n",
        " Example: Batch matrix multiplication. If `T_A` is `(..., m, n)` and `T_B` is `(..., n, p)`, then `einsum('...ij,...jk->...ik', T_A, T_B)` does matrix multiplication for each slice in `...`, outputting `(..., m, p)`. The `...` dimensions must be broadcastable.\n",
        "\n",
        "\n",
        "\n",
        " For `Tensor_A_orig (2,3,4,5)` and `Tensor_B_orig (2,3,5,6)`:\n",
        "\n",
        " If we treat `(2,3)` as batch dimensions (`...`):\n",
        "\n",
        " - `...` maps to `(2,3)`.\n",
        "\n",
        " - `ij` maps to `(4,5)` for `Tensor_A_orig`.\n",
        "\n",
        " - `jk` maps to `(5,6)` for `Tensor_B_orig`. (Here `j` is the common dimension of size 5)\n",
        "\n",
        " - Result `...ik` has shape `(2,3,4,6)`."
      ],
      "metadata": {
        "id": "-ZTtkz50Li0Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOMqX6VNjcm3",
        "outputId": "f91250c3-460e-4cfc-8dc1-3d7d73a33843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of result_ellipsis ('...ij,...jk->...ik'): (2, 3, 4, 6)\n",
            "Dimensions of np.matmul result: (2, 3, 4, 6)\n",
            "Ellipsis and matmul results match: True\n",
            "\n",
            "Dimensions of the result using ellipsis ('...ij,...jk->...ik') on original Tensors: (2, 3, 4, 6)\n",
            "\n",
            "Performance comparison for batch matmul:\n",
            "Einsum with ellipsis timing:\n",
            "6.5 Âµs Â± 1.69 Âµs per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\n",
            "NumPy matmul timing:\n",
            "2.29 Âµs Â± 47 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\n",
            "\n",
            "Shape of np.dot(Tensor_A_orig, Tensor_B_orig): (2, 3, 4, 2, 3, 6)\n"
          ]
        }
      ],
      "source": [
        "A_batch_mm = np.random.rand(2,3,4,5) # Batch of 4x5 matrices\n",
        "B_batch_mm = np.random.rand(2,3,5,6) # Batch of 5x6 matrices\n",
        "\n",
        "result_ellipsis = np.einsum('...ij,...jk->...ik', A_batch_mm, B_batch_mm)\n",
        "print(\"Dimensions of result_ellipsis ('...ij,...jk->...ik'):\", result_ellipsis.shape) # Expected (2,3,4,6)\n",
        "result_matmul = np.matmul(A_batch_mm, B_batch_mm)\n",
        "print(\"Dimensions of np.matmul result:\", result_matmul.shape)\n",
        "print(\"Ellipsis and matmul results match:\", np.allclose(result_ellipsis, result_matmul))\n",
        "\n",
        "result_ellipsis_orig = np.einsum('...ij,...jk->...ik', Tensor_A_orig, Tensor_B_orig)\n",
        "print(\"\\nDimensions of the result using ellipsis ('...ij,...jk->...ik') on original Tensors:\", result_ellipsis_orig.shape)\n",
        "\n",
        "# Performance comparison (actual %timeit commented out)\n",
        "print(\"\\nPerformance comparison for batch matmul:\")\n",
        "print(\"Einsum with ellipsis timing:\")\n",
        "%timeit np.einsum('...ij,...jk->...ik', A_batch_mm, B_batch_mm)\n",
        "print(\"NumPy matmul timing:\")\n",
        "%timeit np.matmul(A_batch_mm, B_batch_mm)\n",
        "\n",
        "print(\"\\nShape of np.dot(Tensor_A_orig, Tensor_B_orig):\", np.dot(Tensor_A_orig, Tensor_B_orig).shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4WHw7kTjcm3"
      },
      "source": [
        " #### Tensor Dot (Specific Contraction)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A specific tensor contraction. For `T_A (i,j,k,l)` and `T_B (i,j,l,m)`.\n",
        "\n",
        " `einsum('ijkl,ijlm->km', T_A, T_B)`\n",
        "\n",
        "\n",
        "\n",
        " -  Inputs: `Tensor_A_orig (2,3,4,5)` and `Tensor_B_orig (2,3,5,6)`.\n",
        "\n",
        "   -  For `Tensor_A_orig`: `i,j,k,l` map to axes `0,1,2,3`.\n",
        "\n",
        "   -  For `Tensor_B_orig`: `i,j,l,m` maps to axes `0,1,2,3` (so `Tensor_B_orig`'s axis 2 is size 5, axis 3 is size 6).\n",
        "\n",
        " -  Output: 2D tensor (indices `k,m`).\n",
        "\n",
        " -  Indices `i,j,l` are summed over (axis 0 of A/B; axis 1 of A/B; axis 3 of A / axis 2 of B).\n",
        "\n",
        " -  Result shape is `(Tensor_A_orig.shape[2], Tensor_B_orig.shape[3])` = `(4,6)`.\n",
        "\n",
        "\n",
        "\n",
        " Equation: $R_{km} = \\sum_i \\sum_j \\sum_l (T_A)_{ijkl} (T_B)_{ijlm}$."
      ],
      "metadata": {
        "id": "HEQAXu_zLoz2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4OLhzQLjcm3",
        "outputId": "8808ca84-4634-4a3f-950b-08d486b30be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of 'ijkl,ijlm->km' result: (4, 6)\n",
            "Dimensions of 'ijkl,mjln->ikmn' result: (2, 4, 2, 6)\n",
            "Complex contraction 'abci,abjd->cijd' result shape: (4, 5, 5, 6)\n"
          ]
        }
      ],
      "source": [
        "result_tensor_dot = np.einsum('ijkl,ijlm->km', Tensor_A_orig, Tensor_B_orig)\n",
        "print(\"Dimensions of 'ijkl,ijlm->km' result:\", result_tensor_dot.shape) # Expected (4,6)\n",
        "\n",
        "# Example from file 2: More complex contraction\n",
        "# A_4d (2,3,4,5), B_4d (2,3,5,6) - same as Tensor_A_orig, Tensor_B_orig\n",
        "complex_contraction = np.einsum('ijkl,mjln->ikmn', Tensor_A_orig, Tensor_B_orig) # Example: T_A(i,j,k,l) T_B(m,j,k,n) -> T_out(i,l,m,n)\n",
        "print(\"Dimensions of 'ijkl,mjln->ikmn' result:\", complex_contraction.shape) # Expected (4,6)\n",
        "\n",
        "complex_example = np.einsum('abci,abjd->cijd', Tensor_A_orig, Tensor_B_orig)\n",
        "print(\"Complex contraction 'abci,abjd->cijd' result shape:\", complex_example.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVudXyg9jcm4"
      },
      "source": [
        " #### Batch Matrix Multiplication (Explicit Batch Index)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For 3D tensors $T_A (b, r_A, c_A)$ and $T_B (b, c_A, c_B)$, perform matrix multiplication for each batch $b$.\n",
        "\n",
        " Equation: $(P_b)_{ik} = \\sum_j (T_A)_{bij} (T_B)_{bjk}$.\n",
        "\n",
        " -  `'bij,bjk->bik'`: `b` is batch, `j` is summed."
      ],
      "metadata": {
        "id": "TckUM_nYLttE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq6j21mhjcm4",
        "outputId": "df925106-3b70-4d51-b964-145b4318010e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch MatMul ('bij,bjk->bik') result shape: (10, 4, 6)\n",
            "Batch MatMul (np.matmul) result shape: (10, 4, 6)\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "Batch_A = np.random.rand(10, 4, 5) # 10 batches of 4x5 matrices\n",
        "Batch_B = np.random.rand(10, 5, 6) # 10 batches of 5x6 matrices\n",
        "\n",
        "result_batch_mm_explicit = np.einsum('bij,bjk->bik', Batch_A, Batch_B)\n",
        "print(\"Batch MatMul ('bij,bjk->bik') result shape:\", result_batch_mm_explicit.shape) # Expected (10,4,6)\n",
        "result_batch_mm_matmul = np.matmul(Batch_A, Batch_B)\n",
        "print(\"Batch MatMul (np.matmul) result shape:\", result_batch_mm_matmul.shape)\n",
        "print(\"Results match:\", np.allclose(result_batch_mm_explicit, result_batch_mm_matmul))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7mhwLx1jcm4"
      },
      "source": [
        " #### Permutation of Tensor Axes\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reorders tensor axes. E.g., $T_{ijk} \\rightarrow T'_{kij}$.\n",
        "\n",
        " Equation: $T'_{kij} = T_{ijk}$.\n",
        "\n",
        " -  `'ijk->kij'`: Axes `i,j,k` are reordered to `k,i,j`."
      ],
      "metadata": {
        "id": "tJDhkl8MLxDK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roZnQuiLjcm4",
        "outputId": "afaa4148-dc4b-481b-912f-7f47986d5976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tensor_permute shape (ijk): (2, 3, 4)\n",
            "Permuted Tensor_permute shape (kij) (einsum 'ijk->kij'): (4, 2, 3)\n",
            "Permuted Tensor_permute shape (kij) (np.transpose(..., axes=(2,0,1))): (4, 2, 3)\n",
            "Results match: True\n"
          ]
        }
      ],
      "source": [
        "Tensor_permute = np.random.rand(2,3,4) # Indices i,j,k\n",
        "print(\"Original Tensor_permute shape (ijk):\", Tensor_permute.shape)\n",
        "permuted_T_einsum = np.einsum('ijk->kij', Tensor_permute)\n",
        "permuted_T_expected = np.transpose(Tensor_permute, axes=(2,0,1))\n",
        "print(\"Permuted Tensor_permute shape (kij) (einsum 'ijk->kij'):\", permuted_T_einsum.shape) # Expected (4,2,3)\n",
        "print(\"Permuted Tensor_permute shape (kij) (np.transpose(..., axes=(2,0,1))):\", permuted_T_expected.shape)\n",
        "print(\"Results match:\", np.array_equal(permuted_T_einsum, permuted_T_expected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k11A7dB1jcm4"
      },
      "source": [
        " #### Broadcasting in Einsum\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Einsum handles broadcasting automatically when dimensions are missing or are 1, similar to other NumPy functions.\n",
        "\n",
        " For example, multiplying a matrix by a vector, where the vector is broadcast across rows or columns."
      ],
      "metadata": {
        "id": "4JST7RGQL4Ik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFGJiLOvjcm4",
        "outputId": "53070cd4-0480-4a88-8997-d87fb27655ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            " [[1 2 3]\n",
            " [4 5 6]]\n",
            "Vector v: [10 20 30]\n",
            "Broadcast multiply (einsum 'ij,j->ij'):\n",
            " [[ 10  40  90]\n",
            " [ 40 100 180]]\n",
            "Expected (A * v[np.newaxis,:]):\n",
            " [[ 10  40  90]\n",
            " [ 40 100 180]]\n",
            "\n",
            "Vector v2: [10 20]\n",
            "Broadcast multiply (einsum 'ij,i->ij'):\n",
            " [[ 10  20  30]\n",
            " [ 80 100 120]]\n",
            "Expected (A * v2[:,np.newaxis]):\n",
            " [[ 10  20  30]\n",
            " [ 80 100 120]]\n"
          ]
        }
      ],
      "source": [
        "A_broadcast_mat = np.array([[1,2,3],[4,5,6]]) # Shape (2,3)\n",
        "v_broadcast_vec = np.array([10,20,30])        # Shape (3,)\n",
        "# Multiply A_broadcast_mat by v_broadcast_vec element-wise for each row of A\n",
        "# 'ij,j->ij': vector j is broadcast to match matrix ij\n",
        "broadcast_result1 = np.einsum('ij,j->ij', A_broadcast_mat, v_broadcast_vec)\n",
        "print(\"Matrix A:\\n\", A_broadcast_mat)\n",
        "print(\"Vector v:\", v_broadcast_vec)\n",
        "print(\"Broadcast multiply (einsum 'ij,j->ij'):\\n\", broadcast_result1)\n",
        "print(\"Expected (A * v[np.newaxis,:]):\\n\", A_broadcast_mat * v_broadcast_vec[np.newaxis,:]) # or A_mat * v_vec\n",
        "\n",
        "v_broadcast_vec2 = np.array([10,20]) # Shape (2,)\n",
        "# Multiply A_broadcast_mat by v_broadcast_vec2 element-wise for each col of A\n",
        "# 'ij,i->ij': vector i is broadcast to match matrix ij\n",
        "broadcast_result2 = np.einsum('ij,i->ij', A_broadcast_mat, v_broadcast_vec2)\n",
        "print(\"\\nVector v2:\", v_broadcast_vec2)\n",
        "print(\"Broadcast multiply (einsum 'ij,i->ij'):\\n\", broadcast_result2)\n",
        "print(\"Expected (A * v2[:,np.newaxis]):\\n\", A_broadcast_mat * v_broadcast_vec2[:,np.newaxis])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqKRjNm9jcm4"
      },
      "source": [
        " ## Tensordot\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`tensordot` also performs tensor contractions, summing over specified axes.\n",
        "\n",
        "\n",
        "\n",
        " ```numpy.tensordot(a, b, axes=2)```\n",
        "\n",
        "\n",
        "\n",
        " ```torch.tensordot(a, b, dims=2, out=None)```\n",
        "\n",
        "\n",
        "\n",
        " The `axes` (NumPy) or `dims` (PyTorch) parameter is key:\n",
        "\n",
        " -  **Integer `N`**: Sums over the last `N` axes of tensor `a` and the first `N` axes of `b`.\n",
        "\n",
        " -  **Tuple of two lists `([a_axes_list], [b_axes_list])`**: Sums over specified axes. `a_axes_list[k]` with `b_axes_list[k]`. Resulting tensor has `a`'s remaining axes, then `b`'s remaining axes, in that order."
      ],
      "metadata": {
        "id": "Mqvt8pdwL95m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCgUHFP4jcm4",
        "outputId": "8f608f5e-b936-4a80-ae3b-287fac57c660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of A_td_torch (PyTorch): torch.Size([2, 3, 4, 5])\n",
            "Dimensions of B_td_torch (PyTorch): torch.Size([2, 3, 5, 6])\n"
          ]
        }
      ],
      "source": [
        "# Using PyTorch tensors for tensordot examples as in original files\n",
        "A_td_torch = torch.rand(2, 3, 4, 5)    # Axes a0,a1,a2,a3\n",
        "B_td_torch = torch.rand(2, 3, 5, 6)    # Axes b0,b1,b2,b3\n",
        "\n",
        "# NumPy counterparts for einsum verification\n",
        "A_td_np = A_td_torch.numpy()\n",
        "B_td_np = B_td_torch.numpy()\n",
        "\n",
        "print(\"Dimensions of A_td_torch (PyTorch):\", A_td_torch.shape)\n",
        "print(\"Dimensions of B_td_torch (PyTorch):\", B_td_torch.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH5ZS10qjcm4"
      },
      "source": [
        " ### Example 1: `tensordot` with specific axes lists\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " `torch.tensordot(A_td_torch, B_td_torch, dims=([0,1,3], [0,1,2]))`\n",
        "\n",
        "\n",
        "\n",
        " -  `A_td_torch` axes `a0,a1,a2,a3` (2,3,4,5); `B_td_torch` axes `b0,b1,b2,b3` (2,3,5,6).\n",
        "\n",
        " -  `dims=([0,1,3], [0,1,2])` contracts:\n",
        "\n",
        "   -  `a0` (size 2) with `b0` (size 2).\n",
        "\n",
        "   -  `a1` (size 3) with `b1` (size 3).\n",
        "\n",
        "   -  `a3` (size 5) with `b2` (size 5).\n",
        "\n",
        " -  Remaining from `A_td_torch`: `a2` (size 4). From `B_td_torch`: `b3` (size 6).\n",
        "\n",
        " -  Result shape: `(A_td_torch.shape[2], B_td_torch.shape[3])` = `(4,6)`.\n",
        "\n",
        "\n",
        "\n",
        " Math: $R_{a_2 b_3} = \\sum_{k_0, k_1, k_2} A_{k_0 k_1 a_2 k_2} B_{k_0 k_1 k_2 b_3}$ (where $k_0, k_1, k_2$ are identified common axes).\n",
        "\n",
        "\n",
        "\n",
        " `einsum` equivalent for $A_{x_0 x_1 x_2 x_3}$ and $B_{y_0 y_1 y_2 y_3}$:\n",
        "\n",
        " Contract $(x_0,y_0)$, $(x_1,y_1)$, $(x_3,y_2)$. Output $x_2, y_3$.\n",
        "\n",
        " Let $x_0=s0, x_1=s1, x_3=s2$. Let $y_0=s0, y_1=s1, y_2=s2$.\n",
        "\n",
        " $A_{s0 s1 x_2 s2}$ and $B_{s0 s1 s2 y_3}$.\n",
        "\n",
        " `'iklm,ikmj->lj'` (if A_axes=[0,1,3] B_axes=[0,1,2])\n",
        "\n",
        " A: $s0=i, s1=k, x2=l, s2=m$. B: $s0=i, s1=k, s2=m, y3=j$.\n",
        "\n",
        " This corresponds to `einsum('abkc,abcl->kl', A_td_np, B_td_np)` if `k` is A's axis 2 and `l` is B's axis 3.\n",
        "\n",
        " Original file 1 used: `einsum('abkc,abcl->kc', A_td_np, B_td_np)` where `k` is A's axis 2, and `c` (representing the last index of B) is B's axis 3. This seems more direct."
      ],
      "metadata": {
        "id": "d3LTgpoBMC4e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib2Q_LyOjcm5",
        "outputId": "dd52a7ca-8ef4-4b1f-a18f-dfa98f0288e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensordot dims=([0,1,3], [0,1,2]) result shape: torch.Size([4, 6])\n",
            "Einsum 'ijAl,ijlk->Ak' result shape: (4, 6)\n",
            "Tensordot and Einsum results match for Example 1. âœ…\n"
          ]
        }
      ],
      "source": [
        "result_td_specific = torch.tensordot(A_td_torch, B_td_torch, dims=([0,1,3], [0,1,2]))\n",
        "print(\"Tensordot dims=([0,1,3], [0,1,2]) result shape:\", result_td_specific.shape) # Expected (4,6)\n",
        "\n",
        "einsum_specific_eq = np.einsum('ijAl,ijlk->Ak', A_td_np, B_td_np) # A(i,j,A,l), B(i,j,m,k) -> A,k\n",
        "\n",
        "print(\"Einsum 'ijAl,ijlk->Ak' result shape:\", einsum_specific_eq.shape)\n",
        "assert np.allclose(result_td_specific.numpy(), einsum_specific_eq), \"Ex1 Tensordot and Einsum results differ!\"\n",
        "print(\"Tensordot and Einsum results match for Example 1. âœ…\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c3lHxsSjcm5"
      },
      "source": [
        " ### Example 2: `tensordot` contracting one pair of axes\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " `torch.tensordot(A_td_torch, B_td_torch, dims=([1], [1]))`\n",
        "\n",
        "\n",
        "\n",
        " -  Contracts axis `a1` (size 3) of `A_td_torch` with axis `b1` (size 3) of `B_td_torch`.\n",
        "\n",
        " -  Remaining from `A_td_torch`: `a0,a2,a3` (sizes 2,4,5). From `B_td_torch`: `b0,b2,b3` (sizes 2,5,6).\n",
        "\n",
        " -  Result shape is `(A_td_torch.shape[0], A_td_torch.shape[2], A_td_torch.shape[3], B_td_torch.shape[0], B_td_torch.shape[2], B_td_torch.shape[3])` which is `(2,4,5,2,5,6)`.\n",
        "\n",
        "\n",
        "\n",
        " Math: $R_{a_0 a_2 a_3 b_0 b_2 b_3} = \\sum_{k} A_{a_0 k a_2 a_3} B_{b_0 k b_2 b_3}$.\n",
        "\n",
        "\n",
        "\n",
        " `einsum` equivalent:\n",
        "\n",
        " A: $iKlm$, B: $xKy z$ (where $K$ is the summed index, axis 1 for both)\n",
        "\n",
        " `einsum('iKlm,xKyz->ilmxz', A_td_np, B_td_np)`"
      ],
      "metadata": {
        "id": "_L7VHNOBMHtK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBtQktJajcm5",
        "outputId": "79f29e09-3eee-464e-fcec-a4509c8dde2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensordot dims=([1],[1]) result shape: torch.Size([2, 4, 5, 2, 5, 6])\n",
            "Einsum 'ijkl,mjno->iklmno' result shape: (2, 4, 5, 2, 5, 6)\n",
            "Tensordot and Einsum results match for Example 2. âœ…\n"
          ]
        }
      ],
      "source": [
        "result_td_ex2 = torch.tensordot(A_td_torch, B_td_torch, dims=([1], [1]))\n",
        "print(\"Tensordot dims=([1],[1]) result shape:\", result_td_ex2.shape) # Expected (2,4,5,2,5,6)\n",
        "\n",
        "# A_td_np (i,K,l,m) = (2,3,4,5)\n",
        "# B_td_np (x,K,y,z) = (2,3,5,6)\n",
        "# Remaining A: i,l,m. Remaining B: x,y,z. Order: A_rem, B_rem\n",
        "einsum_eq_ex2 = np.einsum('ijkl,mjno->iklmno', A_td_np, B_td_np) # Corrected output order for B_rem\n",
        "print(\"Einsum 'ijkl,mjno->iklmno' result shape:\", einsum_eq_ex2.shape)\n",
        "assert np.allclose(result_td_ex2.numpy(), einsum_eq_ex2), \"Ex2 Tensordot and Einsum results differ!\"\n",
        "print(\"Tensordot and Einsum results match for Example 2. âœ…\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEvkzGRhjcm6"
      },
      "source": [
        " #### Understanding Tensordot Axis Specification\n",
        "\n",
        "\n",
        "\n",
        " When we specify `dims=([1], [1])` for `A(a0,a1,a2,a3)` and `B(b0,b1,b2,b3)`:\n",
        "\n",
        " - We contract axis `a1` of the first tensor with axis `b1` of the second tensor.\n",
        "\n",
        " - The remaining axes from A are `(a0,a2,a3)`.\n",
        "\n",
        " - The remaining axes from B are `(b0,b2,b3)`.\n",
        "\n",
        " - `tensordot` concatenates these remaining axes in order: `(a0,a2,a3,b0,b2,b3)`.\n",
        "\n",
        " - So, the final shape is `(A.shape[0], A.shape[2], A.shape[3], B.shape[0], B.shape[2], B.shape[3])`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D47gipcLjcm6"
      },
      "source": [
        " #### More Tensordot Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAbo7m8njcm6",
        "outputId": "facd792e-1781-403f-d2db-db139e985333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A_ex3 shape: torch.Size([2, 3, 4, 5]), B_ex3 shape: torch.Size([4, 5, 6, 7])\n",
            "Tensordot with dims=2 result shape: torch.Size([2, 3, 6, 7])\n",
            "\n",
            "A_td_torch shape: torch.Size([2, 3, 4, 5]), B_td_torch shape: torch.Size([2, 3, 5, 6])\n",
            "Tensordot with dims=([0,3],[0,2]) result shape: torch.Size([3, 4, 3, 6])\n"
          ]
        }
      ],
      "source": [
        "A_ex3 = torch.rand(2,3, 4,5) # Ends with 4,5\n",
        "B_ex3 = torch.rand(4,5, 6,7) # Starts with 4,5\n",
        "result_td_int_dims = torch.tensordot(A_ex3, B_ex3, dims=2)\n",
        "\n",
        "print(f\"A_ex3 shape: {A_ex3.shape}, B_ex3 shape: {B_ex3.shape}\")\n",
        "print(\"Tensordot with dims=2 result shape:\", result_td_int_dims.shape)\n",
        "\n",
        "result_td_multi_noncont = torch.tensordot(A_td_torch, B_td_torch, dims=([0,3],[0,2]))\n",
        "# Remaining A: (a1,a2) -> (3,4)\n",
        "# Remaining B: (b1,b3) -> (3,6)\n",
        "# Result shape: (3,4,3,6)\n",
        "print(f\"\\nA_td_torch shape: {A_td_torch.shape}, B_td_torch shape: {B_td_torch.shape}\")\n",
        "print(\"Tensordot with dims=([0,3],[0,2]) result shape:\", result_td_multi_noncont.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZad7Ahmjcm6"
      },
      "source": [
        " Both `einsum` and `tensordot` are powerful. `einsum` is often more flexible for complex ops or specific output axis orders. `tensordot` is handy for standard tensor dot products over sets of axes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uuWhQJxjcm6"
      },
      "source": [
        " ## Practical Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adj7oQyFjcm6"
      },
      "source": [
        " ### 1. Batch Processing in Neural Networks (Convolution-like operation)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In deep learning, we often need to process batches of data efficiently. `einsum` can express operations like parts of convolutions."
      ],
      "metadata": {
        "id": "lHlp-T6wMVaN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLMeHfW4jcm6",
        "outputId": "63304ef9-06c4-40dc-d014-cefcc44f0293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch shape: (32, 3, 28, 28)\n",
            "Kernel shape: (64, 3, 5, 5)\n",
            "Image patch shape for operation: (32, 3, 5, 5)\n",
            "Convolution-like operation (patch) result shape: (32, 64, 5, 5)\n"
          ]
        }
      ],
      "source": [
        "# Simulate batch of images and a convolutional kernel\n",
        "batch_s, channels_in, height_in, width_in = 32, 3, 28, 28\n",
        "kernel_out_channels, kernel_in_channels, kernel_h, kernel_w = 64, 3, 5, 5\n",
        "\n",
        "images_batch = np.random.rand(batch_s, channels_in, height_in, width_in)\n",
        "kernel_conv = np.random.rand(kernel_out_channels, kernel_in_channels, kernel_h, kernel_w)\n",
        "\n",
        "print(\"Image batch shape:\", images_batch.shape)\n",
        "print(\"Kernel shape:\", kernel_conv.shape)\n",
        "\n",
        "image_patch = images_batch[:, :, :kernel_h, :kernel_w] # Shape: (batch_s, channels_in, kernel_h, kernel_w)\n",
        "print(\"Image patch shape for operation:\", image_patch.shape)\n",
        "\n",
        "conv_like_op = np.einsum('bcij,ocij->boij', image_patch, kernel_conv)\n",
        "print(\"Convolution-like operation (patch) result shape:\", conv_like_op.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jsGTtSjcm6"
      },
      "source": [
        " ### 2. Attention Mechanisms\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Attention mechanisms in transformers use `einsum` (or equivalent batched matrix multiplications) for efficient computation of scores and weighted sums."
      ],
      "metadata": {
        "id": "gWoYcI9fMZVW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmSnfGfojcm6",
        "outputId": "d8c11f59-b0ae-4d15-ceb7-2cc7749163bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention scores shape: (8, 10, 10)\n",
            "Attention weights shape: (8, 10, 10)\n",
            "Attention output shape: (8, 10, 64)\n"
          ]
        }
      ],
      "source": [
        "# Simulate query, key, value matrices for attention\n",
        "seq_len_att, d_model_att = 10, 64\n",
        "batch_size_att = 8\n",
        "\n",
        "Q_att = np.random.rand(batch_size_att, seq_len_att, d_model_att)  # Queries (batch, seq_query, dim_model)\n",
        "K_att = np.random.rand(batch_size_att, seq_len_att, d_model_att)  # Keys   (batch, seq_key,   dim_model)\n",
        "V_att = np.random.rand(batch_size_att, seq_len_att, d_model_att)  # Values (batch, seq_key,   dim_value usually same as d_model)\n",
        "\n",
        "\n",
        "attention_scores = np.einsum('bqd,bkd->bqk', Q_att, K_att)\n",
        "print(\"Attention scores shape:\", attention_scores.shape)\n",
        "\n",
        "attention_weights_simplified = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n",
        "print(\"Attention weights shape:\", attention_weights_simplified.shape)\n",
        "\n",
        "\n",
        "attention_output = np.einsum('bqk,bkd->bqd', attention_weights_simplified, V_att)\n",
        "\n",
        "print(\"Attention output shape:\", attention_output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeuF9h6Rjcm7"
      },
      "source": [
        " ### 3. Statistical Operations\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " `einsum` is useful for computing various statistical measures like covariance matrices."
      ],
      "metadata": {
        "id": "S3yaRqM2MccT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhbhROiLjcm7",
        "outputId": "f0e8d9fd-9870-4c16-de07-b13d653daa85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (samples=100, features=5)\n",
            "Covariance matrix shape (einsum): (5, 5)\n",
            "Covariance matrix shape (numpy): (5, 5)\n",
            "Results match numpy.cov: True\n"
          ]
        }
      ],
      "source": [
        "# Covariance matrix computation\n",
        "num_samples, num_features = 100, 5\n",
        "data_stat = np.random.rand(num_samples, num_features)\n",
        "print(f\"Data shape: (samples={num_samples}, features={num_features})\")\n",
        "\n",
        "# Center the data (subtract mean of each feature)\n",
        "data_centered = data_stat - np.mean(data_stat, axis=0, keepdims=True)\n",
        "\n",
        "cov_matrix_einsum = np.einsum('sn,sm->nm', data_centered, data_centered) / (num_samples - 1)\n",
        "\n",
        "# Verification with np.cov (expects features as rows, so transpose data_stat)\n",
        "cov_matrix_numpy = np.cov(data_stat.T)\n",
        "\n",
        "print(\"Covariance matrix shape (einsum):\", cov_matrix_einsum.shape)\n",
        "print(\"Covariance matrix shape (numpy):\", cov_matrix_numpy.shape)\n",
        "print(\"Results match numpy.cov:\", np.allclose(cov_matrix_einsum, cov_matrix_numpy))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LViqW_LPjcm7"
      },
      "source": [
        " ## Performance Tips and Best Practices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjf85vOojcm7"
      },
      "source": [
        " ### 1. Memory Layout and Performance Considerations\n",
        "\n",
        "\n",
        "\n",
        " For very large arrays, memory layout (Fortran vs. C order) can impact performance. `einsum` operations are generally efficient. Specific performance can depend on the complexity of the contraction and the underlying BLAS/LAPACK libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGXfo08ajcm7",
        "outputId": "4b1453fa-9a1f-4488-8a31-1d7947c10915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance comparison for large matrix multiplication (1000x1000):\n",
            "Matrix multiplication - einsum ('ij,jk->ik'):\n",
            "256 ms Â± 16.7 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
            "Matrix multiplication - einsum with optimization ('ij,jk->ik'):\n",
            "48.4 ms Â± 1.59 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
            "Matrix multiplication - NumPy @ operator:\n",
            "59.4 ms Â± 15.8 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
            "Matrix multiplication - np.dot:\n",
            "47.7 ms Â± 1.02 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "# Example: Matrix multiplication performance\n",
        "large_A_perf = np.random.rand(1000, 1000)\n",
        "large_B_perf = np.random.rand(1000, 1000)\n",
        "\n",
        "print(\"Performance comparison for large matrix multiplication (1000x1000):\")\n",
        "print(\"Matrix multiplication - einsum ('ij,jk->ik'):\")\n",
        "%timeit np.einsum('ij,jk->ik', large_A_perf, large_B_perf)\n",
        "\n",
        "print(\"Matrix multiplication - einsum with optimization ('ij,jk->ik'):\")\n",
        "%timeit np.einsum('ij,jk->ik', large_A_perf, large_B_perf, optimize=True)\n",
        "#\n",
        "print(\"Matrix multiplication - NumPy @ operator:\")\n",
        "%timeit large_A_perf @ large_B_perf\n",
        "#\n",
        "print(\"Matrix multiplication - np.dot:\")\n",
        "%timeit np.dot(large_A_perf, large_B_perf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTlVQ4myjcm7"
      },
      "source": [
        " ### 2. `optimize=True` Parameter in `numpy.einsum`\n",
        "\n",
        "\n",
        "\n",
        " NumPy's `einsum` has an `optimize` parameter that can significantly improve performance for complex operations involving three or more operands by finding an optimal contraction order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHRaeOM5jcm7",
        "outputId": "3bbe6887-2049-4c42-f371-9cc7a207f2fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performance for complex operation ('ijk,jkl,klm->ilm'):\n",
            "Without optimization (optimize=False):\n",
            "147 ms Â± 15.8 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
            "With optimization (optimize=True or default for recent NumPy):\n",
            "5.75 ms Â± 99.8 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "# Complex multi-tensor operation\n",
        "A_complex_perf = np.random.rand(20, 30, 40)\n",
        "B_complex_perf = np.random.rand(30, 40, 50)\n",
        "C_complex_perf = np.random.rand(40, 50, 60)\n",
        "\n",
        "print(\"\\nPerformance for complex operation ('ijk,jkl,klm->ilm'):\")\n",
        "print(\"Without optimization (optimize=False):\")\n",
        "%timeit np.einsum('ijk,jkl,klm->ilm', A_complex_perf, B_complex_perf, C_complex_perf, optimize=False)\n",
        "\n",
        "print(\"With optimization (optimize=True or default for recent NumPy):\")\n",
        "%timeit np.einsum('ijk,jkl,klm->ilm', A_complex_perf, B_complex_perf, C_complex_perf, optimize=True)\n",
        "# For recent NumPy versions, `optimize` might default to an intelligent strategy.\n",
        "# Explicitly setting `optimize=True` (or values like 'greedy', 'optimal') can be beneficial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R4f6d75jcm7"
      },
      "source": [
        " ### 3. Memory Efficiency\n",
        "\n",
        "\n",
        "\n",
        " For very large tensors, consider the memory implications of intermediate arrays that might be created, especially in complex `einsum` expressions if not optimized. `einsum` itself tries to be efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDIOx8MJjcm7",
        "outputId": "8446e568-d040-42c2-ed14-72293a6cff48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of sum_einsum_mem ('ijkl->ik'): (50, 50)\n",
            "Shape of sum_numpy_mem (sum axis=(1,3)): (50, 50)\n",
            "Memory sum operation results match: True\n"
          ]
        }
      ],
      "source": [
        "# Example: Summing along specific axes of a large tensor\n",
        "large_tensor_mem = np.random.rand(50, 50, 50, 50) # Reduced size for quicker execution\n",
        "\n",
        "# Sum over axes 1 (j) and 3 (l)\n",
        "sum_einsum_mem = np.einsum('ijkl->ik', large_tensor_mem)\n",
        "sum_numpy_mem = np.sum(large_tensor_mem, axis=(1, 3))\n",
        "\n",
        "print(\"Shape of sum_einsum_mem ('ijkl->ik'):\", sum_einsum_mem.shape)\n",
        "print(\"Shape of sum_numpy_mem (sum axis=(1,3)):\", sum_numpy_mem.shape)\n",
        "print(\"Memory sum operation results match:\", np.allclose(sum_einsum_mem, sum_numpy_mem))\n",
        "# Both methods are generally efficient for this type of sum reduction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtXMj2WQjcm7"
      },
      "source": [
        " ## Common Patterns\n",
        "\n",
        "\n",
        "\n",
        " Here are some frequently used `einsum` patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bBVAkxLjcm7",
        "outputId": "8f2fb6ee-eed2-47d0-ed3d-2083c4edf1ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common Einsum Patterns:\n",
            "Matrix transpose                        : 'ij->ji'\n",
            "Batch matrix multiplication             : 'bij,bjk->bik'\n",
            "Vector dot product                      : 'i,i->'\n",
            "Vector outer product                    : 'i,j->ij'\n",
            "Matrix trace (sum of diagonal)          : 'ii->'\n",
            "Matrix diagonal extraction              : 'ii->i'\n",
            "Sum all elements (tensor)               : '...->'\n",
            "Sum all elements (matrix)               : 'ij->'\n",
            "Sum matrix along axis 0 (collapse rows) : 'ij->j'\n",
            "Sum matrix along axis 1 (collapse columns): 'ij->i'\n",
            "Element-wise matrix multiplication      : 'ij,ij->ij'\n",
            "Matrix times vector (broadcast over rows): 'ij,j->ij'\n",
            "Vector times matrix (broadcast over columns): 'ij,i->ij'\n",
            "Quadratic form (vector-matrix-vector)   : 'i,ij,j->'\n",
            "Batch diagonal extraction               : '...ii->...i'\n",
            "Batch trace                             : '...ii->...'\n"
          ]
        }
      ],
      "source": [
        "# Pattern collection\n",
        "common_einsum_patterns = {\n",
        "    \"Matrix transpose\": \"'ij->ji'\",\n",
        "    \"Batch matrix multiplication\": \"'bij,bjk->bik'\",\n",
        "    \"Vector dot product\": \"'i,i->'\",\n",
        "    \"Vector outer product\": \"'i,j->ij'\",\n",
        "    \"Matrix trace (sum of diagonal)\": \"'ii->'\",\n",
        "    \"Matrix diagonal extraction\": \"'ii->i'\",\n",
        "    \"Sum all elements (tensor)\": \"'...->'\",\n",
        "    \"Sum all elements (matrix)\": \"'ij->'\",\n",
        "    \"Sum matrix along axis 0 (collapse rows)\": \"'ij->j'\",\n",
        "    \"Sum matrix along axis 1 (collapse columns)\": \"'ij->i'\",\n",
        "    \"Element-wise matrix multiplication\": \"'ij,ij->ij'\",\n",
        "    \"Matrix times vector (broadcast over rows)\": \"'ij,j->ij'\", # Vector 'j' applied to each row 'i'\n",
        "    \"Vector times matrix (broadcast over columns)\": \"'ij,i->ij'\", # Vector 'i' applied to each column 'j'\n",
        "    \"Quadratic form (vector-matrix-vector)\": \"'i,ij,j->'\", # u^T A v\n",
        "    \"Batch diagonal extraction\": \"'...ii->...i'\",\n",
        "    \"Batch trace\": \"'...ii->...'\",\n",
        "}\n",
        "\n",
        "print(\"Common Einsum Patterns:\")\n",
        "for name, pattern_str in common_einsum_patterns.items():\n",
        "    print(f\"{name:40}: {pattern_str}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3JAm_TPjcm7"
      },
      "source": [
        " ## Conclusion\n",
        "\n",
        "\n",
        "\n",
        " This notebook demonstrated the power and flexibility of `einsum` and `tensordot` operations:\n",
        "\n",
        "\n",
        "\n",
        " ### Key Takeaways:\n",
        "\n",
        "\n",
        "\n",
        " 1.  **Concise Notation**: `einsum` provides a powerful and concise domain-specific language for tensor operations using Einstein summation convention.\n",
        "\n",
        " 2.  **Versatility**: Both functions can perform a wide array of operations including, but not limited to, dot products, outer products, transpositions, permutations, and complex tensor contractions.\n",
        "\n",
        " 3.  **Performance**: `einsum` can be highly performant, especially with the `optimize` flag for complex contractions, often matching or exceeding manually chained NumPy operations. `tensordot` is also optimized for its specific task.\n",
        "\n",
        " 4.  **Readability**: While initially cryptic, `einsum` strings can become very readable for those familiar with the notation, clearly expressing the intent of tensor manipulations.\n",
        "\n",
        " 5.  **Framework Support**: Available in both NumPy for CPU-bound tasks and PyTorch (often with GPU acceleration) for deep learning.\n",
        "\n",
        "\n",
        "\n",
        " ### When to Use Each:\n",
        "\n",
        "\n",
        "\n",
        " -   **`einsum`**:\n",
        "\n",
        "     -   For operations requiring flexible control over which indices are summed, kept, or reordered.\n",
        "\n",
        "     -   When performing multiple contractions or a sequence of operations that can be expressed in a single `einsum` string (often more efficient due to optimization).\n",
        "\n",
        "     -   For operations that are hard to express concisely with standard matrix/tensor multiplication and transpose/summation routines (e.g., trace of a batch of matrices, specific diagonal extractions, attention-like mechanisms).\n",
        "\n",
        " -   **`tensordot`**:\n",
        "\n",
        "     -   When you need to contract specific pairs of axes between two tensors.\n",
        "\n",
        "     -   It's a good general-purpose tensor dot product when the `axes` argument clearly defines the contraction.\n",
        "\n",
        "     -   Can be more straightforward than `einsum` if you think in terms of \"sum over these axes from tensor A and these axes from tensor B.\"\n",
        "\n",
        " -   **Standard operators (`@`, `np.dot`, `np.multiply`, `np.sum`, `.T`, etc.)**:\n",
        "\n",
        "     -   For very common and simple operations like element-wise multiplication, matrix multiplication of two matrices, or simple sums/transpositions. These are often the most readable for universally understood operations and are highly optimized.\n",
        "\n",
        "\n",
        "\n",
        " ### Best Practices:\n",
        "\n",
        "\n",
        "\n",
        " 1.  **Clarity**: Add comments to complex `einsum` strings explaining the index notation and the operation being performed.\n",
        "\n",
        " 2.  **Optimization**: For `numpy.einsum` involving three or more tensors, explore the `optimize=True` (or more specific strategies like 'greedy' or 'optimal') argument for potential speedups.\n",
        "\n",
        " 3.  **Profiling**: For performance-critical sections, profile `einsum` against alternative formulations (e.g., using `tensordot` or a sequence of standard NumPy/PyTorch ops).\n",
        "\n",
        " 4.  **Readability vs. Conciseness**: Balance the conciseness of `einsum` with the readability of more verbose standard operations, especially for simpler tasks where `einsum` might be overkill.\n",
        "\n",
        " 5.  **Start Simple**: If new to `einsum`, start by reformulating simple operations (like dot products, transposes) and gradually move to more complex ones.\n",
        "\n",
        "\n",
        "\n",
        " Mastering `einsum` and `tensordot` can significantly enhance your ability to implement complex algorithms in scientific computing and machine learning efficiently and effectively."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SRoSOh8tjcmw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}