{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb-AEda-Qv1C"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn3mrmXXQsiU"
      },
      "source": [
        "Let's start off with a basic node class.\n",
        "We will suggest you hold the data in a np.array\n",
        "Then, create grad so that it is a numpy array of zeros like data.\n",
        "This is so that we can leverage numpy's np.zeros_like() function to copy all types of inputs. Let's also keep track of the prior nodes (parents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdRwzRrZXFPX"
      },
      "outputs": [],
      "source": [
        "#2 mins for this: Transition 1\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Node():\n",
        "    def __init__(self, data, _parents=(), label = \"_\"):\n",
        "        \"\"\"\n",
        "        We want to initialize data, grad, and backward.\n",
        "        where:\n",
        "            data is an array, do we have data?\n",
        "            grad is an array, do we have gradients?\n",
        "            _prev is a set, which one of our inputs holds the previous nodes?\n",
        "        \"\"\"\n",
        "        #TODO:  # Store data as NumPy array, # Gradient initialized to zero,  # Parent nodes in the computation graph\n",
        "        self.data =\n",
        "        self.grad =\n",
        "        self._parents =\n",
        "\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Node({self.label} | data={self.data}, grad={self.grad})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL6hGIcBrhNN"
      },
      "outputs": [],
      "source": [
        "# 5 mins for this: Transition 2\n",
        "\n",
        "def add_node(a_node, b_node):\n",
        "    \"\"\"\n",
        "    Element-wise addition: A + B\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # Produce a new node:\n",
        "    #   Add the input data together to produce the output for the out node.\n",
        "    #   Make sure the output knows what its parents are\n",
        "\n",
        "    # Note:\n",
        "    # - out.grad will be populated later\n",
        "    # - for now, assume out.grad has its proper value for this node.\n",
        "    #\n",
        "    # TODO:\n",
        "    # define a backward function to update a_node's gradients and b_node's gradients\n",
        "    # based on out.grad\n",
        "    #\n",
        "    def add_backward():\n",
        "        pass\n",
        "\n",
        "def mul_node(a_node, b_node):\n",
        "    \"\"\"\n",
        "    Element-wise multiplication: A * B\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # Produce a new node:\n",
        "    #   Make sure the output's data is correct\n",
        "    #   Make sure the output knows what its parents are\n",
        "\n",
        "\n",
        "    # Note:\n",
        "    # - out.grad will be populated later\n",
        "    # - for now, assume out.grad has its proper value for this node.\n",
        "    #\n",
        "    # TODO:\n",
        "    # define a backward function to update a_node's gradients and b_node's gradients\n",
        "    # based on out.grad\n",
        "    #\n",
        "    def mul_backward():\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8DEF8ZecEkN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Testing Node Initialization...\")\n",
        "a_test = Node(3.0)\n",
        "b_test = Node(2.0)\n",
        "\n",
        "assert a_test.data == 3.0, \"Error: a_test.data not set correctly\"\n",
        "assert b_test.data == 2.0, \"Error: b_test.data not set correctly\"\n",
        "assert a_test.grad == 0.0, \"Error: a_test.grad not set correctly\"\n",
        "assert b_test.grad == 0.0, \"Error: b_test.grad not set correctly\"\n",
        "\n",
        "print(\"Passed!\")\n",
        "\n",
        "print(\"Testing Node Addition...\")\n",
        "c_test = add_node(a_test, b_test)\n",
        "assert c_test.data == 5.0, \"Error: add_node DATA not evaluated correctly\"\n",
        "assert c_test.grad == 0.0, \"Error: add_node GRAD not initialized correctly\"\n",
        "print(\"Passed!\")\n",
        "\n",
        "print(\"Testing Node Addition Backward...\")\n",
        "c_test.grad = 1.0\n",
        "c_test.backward_fn()\n",
        "assert a_test.grad == 1.0, \"Error: add_node BACKWARD does not update parent gradients correctly\"\n",
        "assert b_test.grad == 1.0, \"Error: add_node BACKWARD does not update parent gradients correctly\"\n",
        "\n",
        "print(\"Passed!\")\n",
        "\n",
        "a_test = Node(3.0)\n",
        "b_test = Node(2.0)\n",
        "\n",
        "print(\"Testing Node Multiplication...\")\n",
        "d_test = mul_node(a_test, b_test)\n",
        "assert d_test.data == 6.0, \"Error: mul_node DATA not evaluated correctly\"\n",
        "assert d_test.grad == 0.0, \"Error: mul_node GRAD not initialized correctly\"\n",
        "\n",
        "print(\"Passed!\")\n",
        "\n",
        "\n",
        "print(\"Testing Node Multiplication Backward...\")\n",
        "d_test.grad = 1.0\n",
        "d_test.backward_fn()\n",
        "assert a_test.grad == 2.0, \"Error: mul_node BACKWARD does not update parent gradients correctly\"\n",
        "assert b_test.grad == 3.0, \"Error: mul_node BACKWARD does not update parent gradients correctly\"\n",
        "\n",
        "print(\"Passed!\")\n",
        "\n",
        "print(\"All tests passed!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPF1pTIPrju4"
      },
      "outputs": [],
      "source": [
        "#2 mins for this: Transition 2\n",
        "\n",
        "def matmul_node(a_node, b_node):\n",
        "    \"\"\"Matrix multiplication: A @ B\"\"\"\n",
        "    out = None\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ItzMKlCcEkR"
      },
      "outputs": [],
      "source": [
        "a_test = Node(np.array([[1, 2], [3, 4]]))\n",
        "b_test = Node(np.array([[2, 0], [1, 2]]))\n",
        "\n",
        "print(\"Testing Matrix Multiplication...\")\n",
        "c_test = matmul_node(a_test, b_test)\n",
        "\n",
        "assert np.allclose(c_test.data, np.array([[4, 4], [10, 8]])), \"Error: matmul DATA not evaluated correctly\"\n",
        "print(\"Passed!\")\n",
        "\n",
        "print(\"Testing Matrix Multiplication Backward...\")\n",
        "c_test.grad = np.array([[1, 1], [1, 1]])\n",
        "c_test.backward_fn()\n",
        "\n",
        "assert np.allclose(a_test.grad, np.array([[2, 3], [2, 3]])), \"Error: matmul BACKWARD does not update parent gradients correctly\"\n",
        "assert np.allclose(b_test.grad, np.array([[4, 4], [6, 6]])), \"Error: matmul BACKWARD does not update parent gradients correctly\"\n",
        "print(\"Passed!\")\n",
        "\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGTyXsBwPWYF"
      },
      "outputs": [],
      "source": [
        "# 5 mins for this: Transition 2\n",
        "\n",
        "class Node(Node):\n",
        "    def backward(self):\n",
        "        backward_list = []\n",
        "\n",
        "        self.grad = np.ones_like(self.data)  # Placeholder for the gradient of the loss\n",
        "\n",
        "        #TODO: Define a recursive function to build the backward list\n",
        "        def build_backward_list(n):\n",
        "            # TODO: Check if the node is in the backward list\n",
        "            # TODO: Add the node to the backward list\n",
        "            # TODO: iterate through the parents of the node and call the function recursively\n",
        "            pass\n",
        "\n",
        "\n",
        "\n",
        "        build_backward_list(self)\n",
        "        # TODO: print the output backward list if you want to see it!\n",
        "\n",
        "        # TODO: Iterate through the backward list and call the backward functions (if it has parents)\n",
        "        pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONyRhCS9rlAH"
      },
      "outputs": [],
      "source": [
        "a_test = Node(3)\n",
        "b_test = Node(2)\n",
        "\n",
        "c_test = add_node(a_test, b_test)\n",
        "f_test = mul_node(c_test, b_test)\n",
        "\n",
        "\n",
        "print(\"Testing Node BACKWARD...\")\n",
        "f_test.grad = 1.0\n",
        "f_test.backward()\n",
        "\n",
        "print(f\"a_grad: {a_test}\")\n",
        "print(f\"b_grad: {b_test}\")\n",
        "print(f\"c_grad: {c_test}\")\n",
        "print(f\"f_grad: {f_test}\")\n",
        "\n",
        "assert a_test.grad == 2.0, \"Error: BACKWARD does not update parent gradients correctly\"\n",
        "assert b_test.grad == 7.0, \"Error: BACKWARD does not update parent gradients correctly\"\n",
        "assert c_test.grad == 2.0, \"Error: BACKWARD does not update parent gradients correctly\"\n",
        "assert f_test.grad == 1.0, \"Error: BACKWARD does not update parent gradients correctly\"\n",
        "\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Provided primitives\n",
        "\n",
        "def exp_node(a_node):\n",
        "    out = Node(np.exp(a_node.data), (a_node,))\n",
        "    def backward_fn():\n",
        "        a_node.grad += out.grad * out.data\n",
        "    out.backward_fn = backward_fn\n",
        "    return out\n",
        "\n",
        "def log_node(a_node):\n",
        "    out = Node(np.log(a_node.data), (a_node,))\n",
        "    def backward_fn():\n",
        "        a_node.grad += out.grad / a_node.data\n",
        "    out.backward_fn = backward_fn\n",
        "    return out\n",
        "\n",
        "def div_node(a_node, b_node):\n",
        "    out = Node(a_node.data / b_node.data, (a_node, b_node))\n",
        "    def backward_fn():\n",
        "        a_node.grad += out.grad / b_node.data\n",
        "        b_node.grad += -out.grad * a_node.data / (b_node.data**2)\n",
        "    out.backward_fn = backward_fn\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "ebm799l4ELk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softplus_node(x):\n",
        "    \"\"\"\n",
        "    softplus(x) = ln(1 + e^x)\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "xRU7GM-wOmA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a test input node (use a range of values for variety)\n",
        "x = Node(np.array([-2.0, -1.0, 0.0, 1.0, 2.0]))  # A mix of negative and positive values\n",
        "out = softplus_node(x)\n",
        "expected_forward = np.log(1.0 + np.exp(x.data))\n",
        "\n",
        "# Print results for forward pass\n",
        "print(\"Forward Output (softplus(x)):\", out.data)\n",
        "print(\"Expected:\", expected_forward)\n",
        "print(\"Match:\", np.allclose(out.data, expected_forward))\n",
        "\n",
        "\n",
        "out.grad = np.ones_like(out.data)\n",
        "out.backward()\n",
        "expected_grad_x = 1.0 / (1.0 + np.exp(-x.data))  # Sigmoid function\n",
        "\n",
        "# Print results for backward pass (gradients)\n",
        "print(\"\\nGradient wrt Input:\", x.grad)\n",
        "print(\"Expected Gradient:\", expected_grad_x)\n",
        "print(\"Match:\", np.allclose(x.grad, expected_grad_x))\n"
      ],
      "metadata": {
        "id": "4bmruwytOxtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKjcok8qcBb2"
      },
      "outputs": [],
      "source": [
        "def leaky_relu_node(input, alpha=0.01):\n",
        "    \"\"\"\n",
        "    Leaky ReLU activation:\n",
        "        f(x) = x,         if x > 0\n",
        "                alpha * x, otherwise\n",
        "    \"\"\"\n",
        "    # Forward pass\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNYJTcZ2cBhR"
      },
      "outputs": [],
      "source": [
        "# Create a test input node\n",
        "x = Node(np.array([-2.0, -1.0, 0.0, 1.0, 2.0]))  # A mix of negative and positive values\n",
        "alpha = 0.01  # Leaky ReLU slope for negatives\n",
        "\n",
        "# Compute Leaky ReLU\n",
        "out = leaky_relu_node(x, alpha)\n",
        "\n",
        "# Set the output gradient (simulating dL/dout = 1 for all elements)\n",
        "out.grad = np.ones_like(out.data)\n",
        "\n",
        "# Perform backpropagation\n",
        "out.backward()\n",
        "\n",
        "# Expected values\n",
        "expected_forward = np.array([-0.02, -0.01, 0.0, 1.0, 2.0])  # Leaky ReLU applied\n",
        "expected_grad_x = np.array([alpha, alpha, 1.0, 1.0, 1.0])  # Gradients from backprop\n",
        "\n",
        "# Print results\n",
        "print(\"Forward Output:\", out.data)\n",
        "print(\"Expected:\", expected_forward)\n",
        "print(\"Match:\", np.allclose(out.data, expected_forward))\n",
        "\n",
        "print(\"\\nGradient wrt Input:\", x.grad)\n",
        "print(\"Expected Gradient:\", expected_grad_x)\n",
        "print(\"Match:\", np.allclose(x.grad, expected_grad_x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKwFL3hYb9mx"
      },
      "outputs": [],
      "source": [
        "# We'll be nice and give you a div_nodes and exp_nodes primitives :)\n",
        "\n",
        "\n",
        "def tanh_node(x):\n",
        "    \"\"\"\n",
        "    tanh(x) = (e^x - e^-x) / (e^x + e^-x)\n",
        "    \"\"\"\n",
        "   pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRa84bVocemb"
      },
      "outputs": [],
      "source": [
        "# Create a test input node\n",
        "x = Node(np.array([-2.0, -1.0, 0.0, 1.0, 2.0]), label=\"input\")  # A mix of negative and positive values\n",
        "\n",
        "# Compute tanh using our function\n",
        "out = tanh_node(x)\n",
        "\n",
        "# Expected forward values (using NumPy for verification)\n",
        "expected_forward = np.tanh(x.data)\n",
        "\n",
        "# Set the output gradient (simulating dL/dout = 1 for all elements)\n",
        "out.grad = np.ones_like(out.data)\n",
        "\n",
        "# Perform backpropagation\n",
        "out.backward()\n",
        "\n",
        "# # Expected gradients: d/dx tanh(x) = 1 - tanh^2(x)\n",
        "expected_grad_x = 1.0 - expected_forward**2  # (1 - tanh^2(x))\n",
        "print()\n",
        "\n",
        "# Print results\n",
        "print(\"Forward Output:\", out.data)\n",
        "print(\"Expected:\", expected_forward)\n",
        "print(\"Match:\", np.allclose(out.data, expected_forward))\n",
        "\n",
        "print(\"\\nGradient wrt Input:\", x.grad)\n",
        "print(\"Expected Gradient:\", expected_grad_x)\n",
        "print(\"Match:\", np.allclose(x.grad, expected_grad_x))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "idlf24",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}