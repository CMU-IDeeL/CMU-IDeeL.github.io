{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8GAh1XaEzCZ"
      },
      "source": [
        "**Recitation 0: Checkpointing**\n",
        "\n",
        "We will show you how to checkpoint and load your model :D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGTQZCMESiir"
      },
      "source": [
        "**Section 0: Setup**\n",
        "\n",
        "Let's define a quick dummy model, optimizer, and scheduler that we'll be saving and loading :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JWFlVL5oErGf"
      },
      "outputs": [],
      "source": [
        "!pip install wandb --quiet # Install WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI51GXlnLdHs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)\n",
        "\n",
        "os.environ['WANDB_API_KEY'] = \"\"#your key here\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3ZITfNI8Sh6X"
      },
      "outputs": [],
      "source": [
        "# A simple submodule\n",
        "class DummySubmodule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DummySubmodule, self).__init__()\n",
        "        self.layer = nn.Linear(in_features = 32, out_features = 32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "# A simple network\n",
        "class DummyNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DummyNetwork, self).__init__()\n",
        "\n",
        "        self.lower_layer = nn.Sequential(\n",
        "            nn.Linear(in_features = 32, out_features = 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features = 32, out_features = 64),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.upper_layer = nn.Sequential(\n",
        "            nn.Linear(in_features = 64, out_features = 32),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.module1 = DummySubmodule()\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.lower_layer(x)\n",
        "        res = self.upper_layer(res)\n",
        "        res = self.submodule(res)\n",
        "        return res\n",
        "\n",
        "# Declare the model, optimizer, and scheduler\n",
        "model = DummyNetwork().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP28qWKxXkeN"
      },
      "source": [
        "Let's take a look at some of the information we can checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Trj1bxThUpbI",
        "outputId": "5d7db2f3-1aae-4ca2-db04-6e2dd4ab282b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==============================================================\n",
            "Model's state_dict:\n",
            "lower_layer.0.weight \t torch.Size([64, 32])\n",
            "lower_layer.0.bias \t torch.Size([64])\n",
            "lower_layer.2.weight \t torch.Size([64, 32])\n",
            "lower_layer.2.bias \t torch.Size([64])\n",
            "upper_layer.0.weight \t torch.Size([32, 64])\n",
            "upper_layer.0.bias \t torch.Size([32])\n",
            "module1.layer.weight \t torch.Size([32, 32])\n",
            "module1.layer.bias \t torch.Size([32])\n",
            "==============================================================\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'initial_lr': 0.01, 'params': [0, 1, 2, 3, 4, 5, 6, 7]}]\n",
            "==============================================================\n",
            "\n",
            "Scheduler's state_dict:\n",
            "step_size \t 10\n",
            "gamma \t 0.1\n",
            "base_lrs \t [0.01]\n",
            "last_epoch \t 0\n",
            "_step_count \t 1\n",
            "_is_initial \t False\n",
            "_get_lr_called_within_step \t False\n",
            "_last_lr \t [0.01]\n",
            "==============================================================\n"
          ]
        }
      ],
      "source": [
        "# Print model's state_dict\n",
        "print(\"==============================================================\")\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "print(\"==============================================================\")\n",
        "# Print optimizer's state_dict\n",
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
        "print(\"==============================================================\")\n",
        "# Print scheduler's state_dict\n",
        "print(\"\\nScheduler's state_dict:\")\n",
        "for var_name, value in scheduler.state_dict().items():\n",
        "    print(var_name, \"\\t\", value)\n",
        "print(\"==============================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pA_3jeXbNoz"
      },
      "source": [
        "#######################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGmdWx9WSVVM"
      },
      "source": [
        "**Section 1: How to save the checkpoint Saving a checkpoint**\n",
        "\n",
        "Checkpointing locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dxyWbA0CSVEg"
      },
      "outputs": [],
      "source": [
        "# let's pretend we're in the middle of our training\n",
        "epoch = 6 # pretend we're in our 6th epoch\n",
        "loss = 0.78 #pretend this is our model's loss at the moment\n",
        "\n",
        "checkpoint_path=f\"<run_name>_{epoch}.pth\"\n",
        "\n",
        "# Saving your states locally with torch.save\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),   # saving the model state\n",
        "    # if isinstance(model, nn.DataParallel) 'model_state_dict': model.module.state_dict()\n",
        "    'optimizer_state_dict': optimizer.state_dict(),   # saving the optimizer state\n",
        "    'scheduler_state_dict': scheduler.state_dict(),   # saving the scheduler state\n",
        "    'epoch': epoch,\n",
        "    'current_loss': loss\n",
        "    }, checkpoint_path\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternative way of Saving Model Checkpoints Using Helper Function(an example from HW)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative approach: Helper function method (commonly used in HW assignments)\n",
        "# This provides a cleaner, reusable way to save all checkpoint components\n",
        "def save_model(model, optimizer, scheduler, metrics, epoch, path):\n",
        "    \"\"\"\n",
        "    Helper function to save model checkpoint locally.\n",
        "    \n",
        "    Args:\n",
        "        model: The neural network model\n",
        "        optimizer: The optimizer\n",
        "        scheduler: The learning rate scheduler\n",
        "        metrics: Training metrics (e.g., loss, accuracy)\n",
        "        epoch: Current epoch number\n",
        "        path: File path to save the checkpoint\n",
        "    \"\"\"\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         'metric'                   : metrics,\n",
        "         'epoch'                    : epoch},\n",
        "         path)\n",
        "    print(f\"Checkpoint saved to {path}\")\n",
        "\n",
        "# Example usage:\n",
        "metrics = {'loss': 0.78, 'accuracy': 0.92}\n",
        "save_model(model, optimizer, scheduler, metrics, epoch=6, path=\"checkpoint_epoch6.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhCDyi8JX-Dm"
      },
      "source": [
        "Checkpointing and saving to wandb as an artifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCZpMYy4YAtM"
      },
      "outputs": [],
      "source": [
        "# Before the run, you need to have started a run like so....\n",
        "run = wandb.init(\n",
        "    project=\"wandb-quickstart\",\n",
        "    name=\"<run_name>\",\n",
        "    )\n",
        "\n",
        "# ...\n",
        "# ...\n",
        "# ...\n",
        "# Within a training loop (or wherever else you want)....\n",
        "\n",
        "# Option 1:\n",
        "# create artifacts (keeps track of versioning, and is much more organized to work with between collaborators)\n",
        "checkpoint_artifact = wandb.Artifact(\"<run_name>\", type=\"checkpoint\") # You can switch type=\"model if you only want to save a model\"\n",
        "\n",
        "checkpoint_artifact.add_file(checkpoint_path)\n",
        "\n",
        "run.log_artifact(checkpoint_artifact)\n",
        "\n",
        "# Option 2:\n",
        "# directly save the model to wandb\n",
        "wandb.save(checkpoint_path, base_path=os.path.dirname(checkpoint_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybrAu4ghbdCR"
      },
      "source": [
        "#######################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW5LJvUnL7d_"
      },
      "source": [
        "**Section 2\\: Loading a checkpoint file into our current model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsMCPWbWcEiF"
      },
      "source": [
        "Downloading a model from wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFnp3oD5cD0p"
      },
      "outputs": [],
      "source": [
        "# METHOD 1: Download from wandb Artifact\n",
        "# If you need to re-obtain the run, you can do the following....\n",
        "api = wandb.Api()\n",
        "# information can be obtained from the wandb link adddress as follows:\n",
        "# https://wandb.ai/<USERNAME>/<PROJECT_NAME>/runs/<RUN_ID>?nw=nwuser<USERNAME>\n",
        "run = api.run(\"<USERNAME>/<PROJECT_NAME>/<RUN_ID>\")\n",
        "\n",
        "# To retrieve the artifact....\n",
        "# Get the artifact (choose which version of the model you want)\n",
        "artifact = run.use_artifact('<run_name>:latest')\n",
        "# Downloading the artifact\n",
        "artifact_dir = artifact.download()\n",
        "# Loading the model dict\n",
        "checkpoint_dict = torch.load(os.path.join(artifact_dir, '<run_name>'))\n",
        "\n",
        "\n",
        "# METHOD 2: Download the directly saved file from wandb to Local File\n",
        "checkpoint_file = wandb.restore('<run_name>', run_path=\"<USERNAME>/<PROJECT_NAME>/<RUN_ID>\").name\n",
        "checkpoint_dict = torch.load(checkpoint_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0vBwQPkQtMW"
      },
      "source": [
        "Loading a .pth checkpoint file from our local directory to our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmF238XnPw4j",
        "outputId": "9fd6d8ca-6e56-4624-d64f-9427bbae76bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-2bf5796c687f>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_dict = torch.load(checkpoint_path)\n"
          ]
        }
      ],
      "source": [
        "# .pth checkpoint file path can also be obtained from a locally saved .pth file. Or, you can use the checkpoint_dict obtained from the prior wandb artifact download :)\n",
        "checkpoint_path = \"/content/<run_name>_6.pth\"\n",
        "checkpoint_dict = torch.load(checkpoint_path)\n",
        "\n",
        "\n",
        "# Loading model weights\n",
        "# if isinstance(model, nn.DataParallel) model.module.load_state_dict(checkpoint_dict['model_state_dict'])\n",
        "model.load_state_dict(checkpoint_dict['model_state_dict'])\n",
        "# Loading optimizer state\n",
        "optimizer.load_state_dict(checkpoint_dict['optimizer_state_dict'])\n",
        "# Loading the scheduler state\n",
        "scheduler.load_state_dict(checkpoint_dict['scheduler_state_dict'])\n",
        "# find the epoch we left off at\n",
        "current_epoch = checkpoint_dict['epoch']\n",
        "# Find any metrics that might be relevant\n",
        "current_loss = checkpoint_dict['current_loss']\n",
        "\n",
        "# Done!!!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternative way of Loading Model Checkpoints Using Helper Function(an example from HW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Alternative approach: Helper function method (commonly used in HW assignments)\n",
        "# This provides a cleaner, reusable way to load checkpoints with optional components\n",
        "def load_model(model, optimizer=None, scheduler=None, path='./checkpoint.pth'):\n",
        "    \"\"\"\n",
        "    Helper function to load model checkpoint from local storage.\n",
        "    \n",
        "    Args:\n",
        "        model: The neural network model to load weights into\n",
        "        optimizer: The optimizer (optional, can be None for inference)\n",
        "        scheduler: The learning rate scheduler (optional, can be None)\n",
        "        path: File path of the checkpoint to load\n",
        "    \n",
        "    Returns:\n",
        "        model, optimizer, scheduler, epoch, metrics\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    else:\n",
        "        optimizer = None\n",
        "    \n",
        "    if scheduler is not None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    else:\n",
        "        scheduler = None\n",
        "    \n",
        "    epoch = checkpoint['epoch']\n",
        "    metrics = checkpoint['metric']\n",
        "    \n",
        "    print(f\"Checkpoint loaded from {path}\")\n",
        "    print(f\"Resuming from epoch {epoch} with metrics: {metrics}\")\n",
        "    \n",
        "    return model, optimizer, scheduler, epoch, metrics\n",
        "\n",
        "# Example usage 1: Load for resuming training (with optimizer and scheduler)\n",
        "model, optimizer, scheduler, epoch, metrics = load_model(\n",
        "    model, \n",
        "    optimizer=optimizer, \n",
        "    scheduler=scheduler, \n",
        "    path=\"checkpoint_epoch6.pth\"\n",
        ")\n",
        "\n",
        "# Example usage 2: Load for inference only (no optimizer or scheduler needed)\n",
        "model, _, _, epoch, metrics = load_model(\n",
        "    model, \n",
        "    optimizer=None, \n",
        "    scheduler=None, \n",
        "    path=\"checkpoint_epoch6.pth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaM3zpYEgybC"
      },
      "outputs": [],
      "source": [
        "# If you want to load specific parts of your model (in our case, we can load just the lower layers or just the upper layers)\n",
        "specific_weights = { # Creates dictionary of only desired weights\n",
        "    key: value\n",
        "    for key, value in checkpoint_dict['model_state_dict'].items()\n",
        "    if 'lower_layer' in key\n",
        "}\n",
        "\n",
        "model.load_state_dict(specific_weights, strict=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
